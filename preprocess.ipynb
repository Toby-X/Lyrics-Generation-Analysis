{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "from gensim import models\n",
    "from scipy.sparse import lil_matrix, hstack, csr_matrix\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "In this section, we preprocess the data and transform raw text data to matrix form. Then, all data is divided into training set and test set. After that, a dictionary is built upon training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specific_preprocess(doc):\n",
    "    return simple_preprocess(doc,min_len=1)\n",
    "\n",
    "def remove_specific_words(s):\n",
    "    s = re.sub(r\"\\bLyrics\\[.+\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\[.+\\]\",\" \",s)\n",
    "    return s\n",
    "\n",
    "df = pd.read_csv(\"data/billboard_lyrics_genres.csv\")\n",
    "\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(remove_specific_words)\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(remove_stopwords)\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(specific_preprocess)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then delete the songs that are not English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(w):\n",
    "    return w.encode(\"utf-8\").isalpha()\n",
    "\n",
    "def isListEnglish(L):\n",
    "    return all(map(isEnglish,L))\n",
    "\n",
    "df[\"isEnglish\"] = df[\"lyrics\"].map(isListEnglish)\n",
    "df = df.loc[df[\"isEnglish\"],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, perform the same procedure to genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pun(s):\n",
    "    s = re.sub(r\"\\[\\'\",\" \",s)\n",
    "    s = re.sub(r\"\\'\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\'\",\" \",s)\n",
    "    s = re.sub(r\"\\[\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\,\",\" \",s)\n",
    "    s = s.split()\n",
    "    s = [token.lower() for token in s]\n",
    "    return s\n",
    "\n",
    "\n",
    "df[\"genre\"] = df[\"genre\"].map(remove_pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alternative': 146,\n",
      " 'and': 157,\n",
      " 'country': 416,\n",
      " 'dance-pop': 155,\n",
      " 'disco': 147,\n",
      " 'folk': 141,\n",
      " 'funk': 170,\n",
      " 'hard': 102,\n",
      " 'hip': 432,\n",
      " 'hop': 374,\n",
      " 'new': 168,\n",
      " 'pop': 1381,\n",
      " 'r&b': 665,\n",
      " 'rap': 114,\n",
      " 'rock': 1606,\n",
      " 'roll': 111,\n",
      " 'soft': 322,\n",
      " 'soul': 476,\n",
      " 'wave': 109}\n"
     ]
    }
   ],
   "source": [
    "freq_gen = defaultdict(int)\n",
    "for text in df[\"genre\"]:\n",
    "    for token in text:\n",
    "        freq_gen[token] += 1\n",
    "\n",
    "processed_corpus_gen = [[token for token in text if freq_gen[token]>20] for text in df.loc[:,\"genre\"]]\n",
    "dict_gen = corpora.Dictionary(processed_corpus_gen)\n",
    "freq_wanted = {k: v for k,v in freq_gen.items() if v > 100}\n",
    "pprint.pprint(freq_wanted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we can sort out the genre we want is alternative, country, dance, disco, folk, funk, hip-hop, new wave, pop, r&b, rap, rock, soul (soft stands for soft rock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_des = [\"alternative\",\"country\",\"dance\",\"disco\",\"folk\",\"funk\",\"hip\",\"new\",\"pop\",\"r&b\",\"rap\",\"rock\",\"soul\"]\n",
    "gen_des = sorted(gen_des)\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = len(gen_des)\n",
    "dat_gen = lil_matrix((len(df), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    for word in row[\"genre\"]:\n",
    "        for k in range(len(gen_des)):\n",
    "            if re.search(gen_des[k],word):\n",
    "                dat_gen[i,k] = 1\n",
    "            else:\n",
    "                dat_gen[i,k] = 0\n",
    "df[df[\"genre\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_gen = pd.DataFrame.sparse.from_spmatrix(dat_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we should tag the data for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = np.zeros(df.shape[0])\n",
    "\n",
    "bins = [1970,1980,1990,2000,2010,np.inf]\n",
    "\n",
    "labels = [0,1,2,3,4,5]\n",
    "\n",
    "df[\"label\"] = np.where(df[\"year\"] < bins[0], labels[0],\n",
    "                               np.where(df[\"year\"] < bins[1], labels[1],\n",
    "                                        np.where(df[\"year\"] < bins[2], labels[2],\n",
    "                                                 np.where(df[\"year\"] < bins[3], labels[3],\n",
    "                                                          np.where(df[\"year\"] < bins[4], labels[4], labels[5])))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, data is split to training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(515)\n",
    "idx = np.repeat(range(10),len(df.iloc[:,0])//10+1)\n",
    "df[\"idx\"] = np.random.choice(idx[range(len(df.iloc[:,0]))],size=len(df.iloc[:,0]))\n",
    "df_train = df.loc[df[\"idx\"]!=0,:]\n",
    "df_test = df.loc[df[\"idx\"]==0,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dictionary based on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_18332\\4121515207.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n"
     ]
    }
   ],
   "source": [
    "freq = defaultdict(int)\n",
    "for text in df_train[\"lyrics\"]:\n",
    "    for token in text:\n",
    "        freq[token] += 1\n",
    "\n",
    "processed_corpus = [[token for token in text if freq[token]>20] for text in df_train.loc[:,\"lyrics\"]]\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "df_train[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_train = lil_matrix((len(df_train), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"freq_count\"]]\n",
    "    values = [value for _, value in row[\"freq_count\"]]\n",
    "    dat_train[i, indices] = values\n",
    "df_train[df_train[\"freq_count\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_train = pd.DataFrame.sparse.from_spmatrix(dat_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, perform the same procedure to test set with the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_18332\\2217317512.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n"
     ]
    }
   ],
   "source": [
    "df_test = df.loc[df[\"idx\"]==0,:]\n",
    "processed_corpus = [[token for token in text if freq[token]>20] for text in df_test.loc[:,\"lyrics\"]]\n",
    "df_test[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_test = lil_matrix((len(df_test), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"freq_count\"] if count < num_cols]\n",
    "    values = [value for count, value in row[\"freq_count\"] if count < num_cols and value!=0]\n",
    "    dat_test[i, indices] = values\n",
    "df_test[df_test[\"freq_count\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_test = pd.DataFrame.sparse.from_spmatrix(dat_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_18332\\1318062561.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"tfidf\"]=tfidf[df_train[\"lyrics\"].map(dictionary.doc2bow)]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = list(df_train[\"freq_count\"])\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "df_train[\"tfidf\"]=tfidf[df_train[\"lyrics\"].map(dictionary.doc2bow)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_tfidf_train = lil_matrix((len(df_train), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"tfidf\"]]\n",
    "    values = [value for _, value in row[\"tfidf\"]]\n",
    "    dat_tfidf_train[i, indices] = values\n",
    "df_train[df_train[\"tfidf\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_tfidf_train = pd.DataFrame.sparse.from_spmatrix(dat_tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dat_tfidf_train.loc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_18332\\2525057283.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"tfidf\"]=tfidf[df_test[\"lyrics\"].map(dictionary.doc2bow)]\n"
     ]
    }
   ],
   "source": [
    "df_test[\"tfidf\"]=tfidf[df_test[\"lyrics\"].map(dictionary.doc2bow)]\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_tfidf_test = lil_matrix((len(df_test), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"tfidf\"] if count < num_cols]\n",
    "    values = [value for count, value in row[\"tfidf\"] if count < num_cols and value != 0]\n",
    "    dat_tfidf_test[i, indices] = values\n",
    "df_test[df_test[\"tfidf\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_tfidf_test = pd.DataFrame.sparse.from_spmatrix(dat_tfidf_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed Data\n",
    "The data processed are diveded into the blow categories:\n",
    "\n",
    "Original word frequency + genre\n",
    "\n",
    "TF-IDF word frequency + genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_gen = dat_gen.reset_index()\n",
    "df = df.reset_index(drop=True)\n",
    "dat_gen_train = dat_gen.loc[df[\"idx\"]!=0,:].reset_index(drop=True)\n",
    "dat_gen_test = dat_gen.loc[df[\"idx\"]==0,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = csr_matrix(df_train.loc[:,\"label\"]).transpose()\n",
    "test_label = csr_matrix(df_test.loc[:,\"label\"]).transpose()\n",
    "\n",
    "gen_train = csr_matrix(dat_gen_train.loc[:,0:])\n",
    "lyrics_train = csr_matrix(dat_train.loc[:,0:])\n",
    "data_train = hstack([gen_train, lyrics_train,train_label])\n",
    "data_train = pd.DataFrame.sparse.from_spmatrix(data_train)\n",
    "\n",
    "gen_test = csr_matrix(dat_gen_test.loc[:,0:])\n",
    "lyrics_test = csr_matrix(dat_test.loc[:,0:])\n",
    "data_test = hstack([gen_test, lyrics_test,test_label])\n",
    "data_test = pd.DataFrame.sparse.from_spmatrix(data_test)\n",
    "\n",
    "\n",
    "lyrics_tfidf_train = csr_matrix(dat_tfidf_train.loc[:,0:])\n",
    "data_tfidf_train = hstack([gen_train,lyrics_tfidf_train,train_label])\n",
    "data_tfidf_train = pd.DataFrame.sparse.from_spmatrix(data_tfidf_train)\n",
    "\n",
    "lyrics_tfidf_test = csr_matrix(dat_tfidf_test.loc[:,0:])\n",
    "data_tfidf_test = hstack([gen_test,lyrics_tfidf_test,test_label])\n",
    "data_tfidf_test = pd.DataFrame.sparse.from_spmatrix(data_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_name = [dictionary[i] for i in range(max(dictionary.keys())+1)]\n",
    "word_name = ['index']+gen_des + word_name\n",
    "data_tfidf_test.columns = word_name\n",
    "data_tfidf_train.columns = word_name\n",
    "data_train.columns = word_name\n",
    "data_test.columns = word_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_18332\\847117177.py:1: FutureWarning: The behavior of .astype from SparseDtype to a non-sparse dtype is deprecated. In a future version, this will return a non-sparse array with the requested dtype. To retain the old behavior, use `obj.astype(SparseDtype(dtype))`\n",
      "  data_tfidf_train.to_csv(\"data/train_tfidf_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "data_tfidf_train.to_csv(\"data/train_tfidf_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_18332\\1944018287.py:1: FutureWarning: The behavior of .astype from SparseDtype to a non-sparse dtype is deprecated. In a future version, this will return a non-sparse array with the requested dtype. To retain the old behavior, use `obj.astype(SparseDtype(dtype))`\n",
      "  data_tfidf_test.to_csv(\"data/test_tfidf_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "data_tfidf_test.to_csv(\"data/test_tfidf_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
