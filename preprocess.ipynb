{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "from gensim import models\n",
    "from scipy.sparse import lil_matrix, hstack, csr_matrix\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "In this section, we preprocess the data and transform raw text data to matrix form. Then, all data is divided into training set and test set. After that, a dictionary is built upon training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specific_preprocess(doc):\n",
    "    return simple_preprocess(doc,min_len=1)\n",
    "\n",
    "def remove_specific_words(s):\n",
    "    s = re.sub(r\"\\bLyrics\\[.+\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\[.+\\]\",\" \",s)\n",
    "    return s\n",
    "\n",
    "df = pd.read_csv(\"data/billboard_lyrics_genres.csv\")\n",
    "\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(remove_specific_words)\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(remove_stopwords)\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(specific_preprocess)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then delete the songs that are not English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(w):\n",
    "    return w.encode(\"utf-8\").isalpha()\n",
    "\n",
    "def isListEnglish(L):\n",
    "    return all(map(isEnglish,L))\n",
    "\n",
    "df[\"isEnglish\"] = df[\"lyrics\"].map(isListEnglish)\n",
    "df = df.loc[df[\"isEnglish\"],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, perform the same procedure to genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pun(s):\n",
    "    s = re.sub(r\"\\[\\'\",\" \",s)\n",
    "    s = re.sub(r\"\\'\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\'\",\" \",s)\n",
    "    s = re.sub(r\"\\[\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\,\",\" \",s)\n",
    "    s = s.split()\n",
    "    s = [token.lower() for token in s]\n",
    "    return s\n",
    "\n",
    "\n",
    "df[\"genre\"] = df[\"genre\"].map(remove_pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alternative': 146,\n",
      " 'and': 157,\n",
      " 'country': 416,\n",
      " 'dance-pop': 155,\n",
      " 'disco': 147,\n",
      " 'folk': 141,\n",
      " 'funk': 170,\n",
      " 'hard': 102,\n",
      " 'hip': 432,\n",
      " 'hop': 374,\n",
      " 'new': 168,\n",
      " 'pop': 1381,\n",
      " 'r&b': 665,\n",
      " 'rap': 114,\n",
      " 'rock': 1606,\n",
      " 'roll': 111,\n",
      " 'soft': 322,\n",
      " 'soul': 476,\n",
      " 'wave': 109}\n"
     ]
    }
   ],
   "source": [
    "freq_gen = defaultdict(int)\n",
    "for text in df[\"genre\"]:\n",
    "    for token in text:\n",
    "        freq_gen[token] += 1\n",
    "\n",
    "processed_corpus_gen = [[token for token in text if freq_gen[token]>20] for text in df.loc[:,\"genre\"]]\n",
    "dict_gen = corpora.Dictionary(processed_corpus_gen)\n",
    "freq_wanted = {k: v for k,v in freq_gen.items() if v > 100}\n",
    "pprint.pprint(freq_wanted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we can sort out the genre we want is alternative, country, dance, disco, folk, funk, hip-hop, new wave, pop, r&b, rap, rock, soul (soft stands for soft rock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_des = [\"alternative\",\"country\",\"dance\",\"disco\",\"folk\",\"funk\",\"hip\",\"new\",\"pop\",\"r&b\",\"rap\",\"rock\",\"soul\"]\n",
    "gen_des = sorted(gen_des)\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = len(gen_des)\n",
    "dat_gen = lil_matrix((len(df), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    for word in row[\"genre\"]:\n",
    "        for k in range(len(gen_des)):\n",
    "            if re.search(gen_des[k],word):\n",
    "                dat_gen[i,k] = 1\n",
    "            else:\n",
    "                dat_gen[i,k] = 0\n",
    "df[df[\"genre\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_gen = pd.DataFrame.sparse.from_spmatrix(dat_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we should tag the data for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = np.zeros(df.shape[0])\n",
    "\n",
    "bins = [1970,1980,1990,2000,2010,np.inf]\n",
    "\n",
    "labels = [0,1,2,3,4,5]\n",
    "\n",
    "df[\"label\"] = np.where(df[\"year\"] < bins[0], labels[0],\n",
    "                               np.where(df[\"year\"] < bins[1], labels[1],\n",
    "                                        np.where(df[\"year\"] < bins[2], labels[2],\n",
    "                                                 np.where(df[\"year\"] < bins[3], labels[3],\n",
    "                                                          np.where(df[\"year\"] < bins[4], labels[4], labels[5])))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, data is split to training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(515)\n",
    "idx = np.repeat(range(10),len(df.iloc[:,0])//10+1)\n",
    "df[\"idx\"] = np.random.choice(idx[range(len(df.iloc[:,0]))],size=len(df.iloc[:,0]))\n",
    "df_train = df.loc[df[\"idx\"]!=0,:]\n",
    "df_test = df.loc[df[\"idx\"]==0,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dictionary based on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_17244\\4121515207.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n"
     ]
    }
   ],
   "source": [
    "freq = defaultdict(int)\n",
    "for text in df_train[\"lyrics\"]:\n",
    "    for token in text:\n",
    "        freq[token] += 1\n",
    "\n",
    "processed_corpus = [[token for token in text if freq[token]>20] for text in df_train.loc[:,\"lyrics\"]]\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "df_train[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_train = lil_matrix((len(df_train), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"freq_count\"]]\n",
    "    values = [value for _, value in row[\"freq_count\"]]\n",
    "    dat_train[i, indices] = values\n",
    "df_train[df_train[\"freq_count\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_train = pd.DataFrame.sparse.from_spmatrix(dat_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, perform the same procedure to test set with the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_17244\\2217317512.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n"
     ]
    }
   ],
   "source": [
    "df_test = df.loc[df[\"idx\"]==0,:]\n",
    "processed_corpus = [[token for token in text if freq[token]>20] for text in df_test.loc[:,\"lyrics\"]]\n",
    "df_test[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_test = lil_matrix((len(df_test), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"freq_count\"] if count < num_cols]\n",
    "    values = [value for count, value in row[\"freq_count\"] if count < num_cols and value!=0]\n",
    "    dat_test[i, indices] = values\n",
    "df_test[df_test[\"freq_count\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_test = pd.DataFrame.sparse.from_spmatrix(dat_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_17244\\1318062561.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"tfidf\"]=tfidf[df_train[\"lyrics\"].map(dictionary.doc2bow)]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = list(df_train[\"freq_count\"])\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "df_train[\"tfidf\"]=tfidf[df_train[\"lyrics\"].map(dictionary.doc2bow)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_tfidf_train = lil_matrix((len(df_train), num_cols), dtype=np.float64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"tfidf\"]]\n",
    "    values = [value for _, value in row[\"tfidf\"]]\n",
    "    dat_tfidf_train[i, indices] = values\n",
    "df_train[df_train[\"tfidf\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_tfidf_train = pd.DataFrame.sparse.from_spmatrix(dat_tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_17244\\3654324900.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"tfidf\"]=tfidf[df_test[\"lyrics\"].map(dictionary.doc2bow)]\n"
     ]
    }
   ],
   "source": [
    "df_test[\"tfidf\"]=tfidf[df_test[\"lyrics\"].map(dictionary.doc2bow)]\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_tfidf_test = lil_matrix((len(df_test), num_cols), dtype=np.float64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"tfidf\"] if count < num_cols]\n",
    "    values = [value for count, value in row[\"tfidf\"] if count < num_cols and value != 0]\n",
    "    dat_tfidf_test[i, indices] = values\n",
    "df_test[df_test[\"tfidf\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_tfidf_test = pd.DataFrame.sparse.from_spmatrix(dat_tfidf_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed Data\n",
    "The data processed are diveded into the blow categories:\n",
    "\n",
    "Original word frequency + genre\n",
    "\n",
    "TF-IDF word frequency + genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_gen = dat_gen.reset_index()\n",
    "df = df.reset_index(drop=True)\n",
    "dat_gen_train = dat_gen.loc[df[\"idx\"]!=0,:].reset_index(drop=True)\n",
    "dat_gen_test = dat_gen.loc[df[\"idx\"]==0,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = csr_matrix(df_train.loc[:,\"label\"]).transpose()\n",
    "test_label = csr_matrix(df_test.loc[:,\"label\"]).transpose()\n",
    "\n",
    "gen_train = csr_matrix(dat_gen_train.loc[:,0:])\n",
    "lyrics_train = csr_matrix(dat_train.loc[:,0:])\n",
    "data_train = hstack([gen_train, lyrics_train,train_label])\n",
    "data_train = pd.DataFrame.sparse.from_spmatrix(data_train)\n",
    "\n",
    "gen_test = csr_matrix(dat_gen_test.loc[:,0:])\n",
    "lyrics_test = csr_matrix(dat_test.loc[:,0:])\n",
    "data_test = hstack([gen_test, lyrics_test,test_label])\n",
    "data_test = pd.DataFrame.sparse.from_spmatrix(data_test)\n",
    "\n",
    "\n",
    "lyrics_tfidf_train = csr_matrix(dat_tfidf_train.loc[:,0:])\n",
    "data_tfidf_train = hstack([gen_train,lyrics_tfidf_train,train_label])\n",
    "data_tfidf_train = pd.DataFrame.sparse.from_spmatrix(data_tfidf_train)\n",
    "\n",
    "lyrics_tfidf_test = csr_matrix(dat_tfidf_test.loc[:,0:])\n",
    "data_tfidf_test = hstack([gen_test,lyrics_tfidf_test,test_label])\n",
    "data_tfidf_test = pd.DataFrame.sparse.from_spmatrix(data_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_name = [dictionary[i] for i in range(max(dictionary.keys())+1)]\n",
    "word_name = gen_des + word_name + ['label']\n",
    "data_tfidf_test.columns = word_name\n",
    "data_tfidf_train.columns = word_name\n",
    "data_train.columns = word_name\n",
    "data_test.columns = word_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_17244\\86860707.py:1: FutureWarning: The behavior of .astype from SparseDtype to a non-sparse dtype is deprecated. In a future version, this will return a non-sparse array with the requested dtype. To retain the old behavior, use `obj.astype(SparseDtype(dtype))`\n",
      "  data_tfidf_train.to_csv(\"data/train_tfidf_data.csv\")\n",
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_17244\\86860707.py:2: FutureWarning: The behavior of .astype from SparseDtype to a non-sparse dtype is deprecated. In a future version, this will return a non-sparse array with the requested dtype. To retain the old behavior, use `obj.astype(SparseDtype(dtype))`\n",
      "  data_tfidf_test.to_csv(\"data/test_tfidf_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "# data_tfidf_train.to_csv(\"data/train_tfidf_data.csv\")\n",
    "# data_tfidf_test.to_csv(\"data/test_tfidf_data.csv\")\n",
    "# data_train = hstack([lyrics_train,train_label])\n",
    "# data_train = pd.DataFrame.sparse.from_spmatrix(data_train)\n",
    "# data_test = hstack([lyrics_test,test_label])\n",
    "# data_test = pd.DataFrame.sparse.from_spmatrix(data_test)\n",
    "# word_name = [dictionary[i] for i in range(max(dictionary.keys())+1)]\n",
    "# word_name = word_name+['label']\n",
    "# data_train.columns = word_name\n",
    "# data_test.columns = word_name\n",
    "\n",
    "# data_train.to_csv(\"data/train_data_all.csv\")\n",
    "# data_test.to_csv(\"data/test_data_all.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540772532188841\n",
      "4.954935622317596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mr = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],np.array(df_train[\"label\"]))\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,:(data_tfidf_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == df_test[\"label\"])/len(pred))\n",
    "print(np.mean((pred-df_test[\"label\"])**2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct Use of Multinomial Logistic Regression's Performance is very bad.\n",
    "\n",
    "Hence, we consider here multi logistic scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "label_60 = df_train[\"label\"]==0\n",
    "mr60 = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_60)\n",
    "label_70 = df_train[\"label\"]==1\n",
    "mr70 = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_70)\n",
    "label_80 = df_train[\"label\"]==2\n",
    "mr80 = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_80)\n",
    "label_90 = df_train[\"label\"]==3\n",
    "mr90 = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_90)\n",
    "label_00 = df_train[\"label\"]==4\n",
    "mr00 = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_00)\n",
    "label_10 = df_train[\"label\"]==5\n",
    "mr10 = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_10)\n",
    "\n",
    "def predict_multi_logit(df):\n",
    "    prob60 = mr60.predict_log_proba(df)[:,1]\n",
    "    prob70 = mr70.predict_log_proba(df)[:,1]\n",
    "    prob80 = mr80.predict_log_proba(df)[:,1]\n",
    "    prob90 = mr90.predict_log_proba(df)[:,1]\n",
    "    prob00 = mr00.predict_log_proba(df)[:,1]\n",
    "    prob10 = mr10.predict_log_proba(df)[:,1]\n",
    "    prob = pd.DataFrame([prob60,prob70,prob80,prob90,prob00,prob10])\n",
    "    return prob.apply(np.argmax,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_multi_logit(data_tfidf_test.iloc[:,:(data_tfidf_train.shape[1]-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24      0\n",
       "45      0\n",
       "49      0\n",
       "65      0\n",
       "66      0\n",
       "       ..\n",
       "5303    5\n",
       "5349    5\n",
       "5400    5\n",
       "5404    5\n",
       "5419    5\n",
       "Name: label, Length: 466, dtype: int32"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540772532188841\n"
     ]
    }
   ],
   "source": [
    "print(sum(np.array(pred) == np.array(df_test[\"label\"]))/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8218884120171673\n"
     ]
    }
   ],
   "source": [
    "label_60 = df_train[\"label\"]==0\n",
    "label_60_test = df_test[\"label\"]==0\n",
    "\n",
    "mr = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_60)\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,:(data_tfidf_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == label_60_test)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8347639484978541\n"
     ]
    }
   ],
   "source": [
    "label_70 = df_train[\"label\"]==1\n",
    "label_70_test = df_test[\"label\"]==1\n",
    "\n",
    "mr = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_70)\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,:(data_tfidf_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == label_70_test)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8540772532188842\n"
     ]
    }
   ],
   "source": [
    "label_80 = df_train[\"label\"]==2\n",
    "label_80_test = df_test[\"label\"]==2\n",
    "\n",
    "mr = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_80)\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,:(data_tfidf_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == label_80_test)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8326180257510729\n"
     ]
    }
   ],
   "source": [
    "label_90 = df_train[\"label\"]==3\n",
    "label_90_test = df_test[\"label\"]==3\n",
    "\n",
    "mr = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_90)\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,:(data_tfidf_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == label_90_test)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8583690987124464\n"
     ]
    }
   ],
   "source": [
    "label_00 = df_train[\"label\"]==4\n",
    "label_00_test = df_test[\"label\"]==4\n",
    "\n",
    "mr = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_00)\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,:(data_tfidf_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == label_00_test)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8283261802575107\n"
     ]
    }
   ],
   "source": [
    "label_10 = df_train[\"label\"]==5\n",
    "label_10_test = df_test[\"label\"]==5\n",
    "\n",
    "mr = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],label_10)\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,:(data_tfidf_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == label_10_test)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.84959223,\n",
       "       -2.15934671, -1.05760951, -2.15934671, -4.62434577, -1.05760951,\n",
       "       -1.45467241, -2.54483324, -1.05760951, -1.05760951, -1.45467241,\n",
       "       -1.45467241, -1.45467241, -1.05760951, -3.24836749, -1.05760951,\n",
       "       -1.84959223, -1.45467241, -2.15934671, -1.45467241, -1.05760951,\n",
       "       -3.40862264, -3.40862264, -3.24836749, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -3.40862264, -1.05760951, -3.24836749, -1.05760951,\n",
       "       -2.33079205, -3.40862264, -1.05760951, -3.24836749, -2.15934671,\n",
       "       -1.05760951, -2.15934671, -3.24836749, -1.05760951, -4.62434577,\n",
       "       -2.15934671, -3.40862264, -3.40862264, -2.33079205, -3.40862264,\n",
       "       -1.05760951, -3.40862264, -1.05760951, -3.40862264, -3.40862264,\n",
       "       -3.40862264, -3.40862264, -3.40862264, -2.15934671, -2.54483324,\n",
       "       -2.15934671, -2.15934671, -2.33079205, -3.40862264, -1.84959223,\n",
       "       -2.15934671, -1.05760951, -3.20726203, -2.15934671, -3.40862264,\n",
       "       -3.40862264, -3.40862264, -2.15934671, -3.40862264, -3.24836749,\n",
       "       -2.15934671, -3.40862264, -1.45467241, -3.24836749, -1.05760951,\n",
       "       -2.15934671, -2.33079205, -1.05760951, -3.24836749, -3.40862264,\n",
       "       -3.40862264, -3.40862264, -3.40862264, -3.12892925, -1.05760951,\n",
       "       -1.45467241, -3.40862264, -2.15934671, -2.15934671, -3.40862264,\n",
       "       -2.54483324, -2.15934671, -2.54483324, -3.40862264, -2.15934671,\n",
       "       -2.15934671, -3.40862264, -2.54483324, -3.40862264, -2.33079205,\n",
       "       -4.34290709, -3.40862264, -2.15934671, -7.57632754, -1.05760951,\n",
       "       -3.40862264, -2.15934671, -2.33079205, -1.05760951, -2.15934671,\n",
       "       -2.33079205, -3.40862264, -1.05760951, -2.54483324, -1.45467241,\n",
       "       -3.40862264, -1.84959223, -3.40862264, -3.24836749, -2.54483324,\n",
       "       -1.05760951, -3.40862264, -3.20726203, -1.45467241, -3.20726203,\n",
       "       -3.40862264, -3.40862264, -1.05760951, -2.15934671, -3.40862264,\n",
       "       -3.40862264, -3.40862264, -1.05760951, -3.24836749, -3.40862264,\n",
       "       -1.05760951, -3.40862264, -3.40862264, -3.20726203, -2.15934671,\n",
       "       -1.05760951, -3.20726203, -3.20726203, -3.24836749, -3.20726203,\n",
       "       -3.20726203, -1.05760951, -3.20726203, -3.40862264, -2.54483324,\n",
       "       -1.45467241, -3.20726203, -3.40862264, -3.40862264, -3.40862264,\n",
       "       -2.54483324, -3.24836749, -3.20726203, -3.20726203, -2.54483324,\n",
       "       -3.40862264, -3.40862264, -3.24836749, -3.95216344, -3.40862264,\n",
       "       -3.40862264, -2.15934671, -2.15934671, -1.05760951, -3.40862264,\n",
       "       -1.45467241, -1.45467241, -9.14787356, -1.05760951, -3.40862264,\n",
       "       -3.40862264, -8.956284  , -3.40862264, -3.40862264, -1.05760951,\n",
       "       -3.40862264, -3.40862264, -5.00478254, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -3.40862264, -3.04056207, -3.40862264, -3.40862264,\n",
       "       -3.40862264, -3.40862264, -3.40862264, -3.40862264, -3.40862264,\n",
       "       -3.40862264, -1.05760951, -3.24836749, -2.15934671, -3.04056207,\n",
       "       -2.33079205, -3.40862264, -1.05760951, -2.15934671, -3.40862264,\n",
       "       -3.40862264, -3.20726203, -3.40862264, -1.05760951, -3.40862264,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -2.54483324, -4.78906283,\n",
       "       -1.05760951, -4.34290709, -1.05760951, -1.05760951, -2.54483324,\n",
       "       -1.05760951, -2.15934671, -3.40862264, -3.40862264, -1.05760951,\n",
       "       -2.15934671, -2.15934671, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.45467241, -1.05760951, -3.40862264, -3.40862264, -1.05760951,\n",
       "       -2.54483324, -3.40862264, -1.05760951, -1.45467241, -5.21253982,\n",
       "       -2.54483324, -2.15934671, -1.05760951, -2.33079205, -2.54483324,\n",
       "       -3.24836749, -1.45467241, -3.40862264, -1.05760951, -3.40862264,\n",
       "       -1.05760951, -2.54483324, -2.54483324, -3.04056207, -2.54483324,\n",
       "       -3.40862264, -1.05760951, -2.15934671, -3.24836749, -1.77213495,\n",
       "       -2.54483324, -2.54483324, -2.15934671, -1.05760951, -2.15934671,\n",
       "       -2.54483324, -2.15934671, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -2.54483324, -6.44817289, -1.05760951, -3.24836749,\n",
       "       -3.40862264, -5.00478254, -1.45467241, -3.88949519, -3.40862264,\n",
       "       -2.15934671, -2.15934671, -2.15934671, -2.15934671, -1.05760951,\n",
       "       -1.45467241, -2.15934671, -2.54483324, -2.15934671, -1.05760951,\n",
       "       -1.05760951, -2.54483324, -2.54483324, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -2.54483324, -2.15934671, -2.15934671, -2.54483324,\n",
       "       -2.54483324, -3.24836749, -1.05760951, -2.54483324, -2.54483324,\n",
       "       -2.54483324, -1.45467241, -1.45467241, -1.05760951, -2.15934671,\n",
       "       -1.05760951, -3.40862264, -1.05760951, -2.54483324, -5.04730398,\n",
       "       -2.15934671, -1.05760951, -1.68071704, -2.54483324, -1.05760951,\n",
       "       -1.05760951, -2.15934671, -1.05760951, -2.54483324, -3.40862264,\n",
       "       -2.54483324, -2.15934671, -3.40862264, -1.05760951, -2.15934671,\n",
       "       -1.05760951, -1.45467241, -2.15934671, -3.24836749, -1.05760951,\n",
       "       -1.68071704, -2.15934671, -1.05760951, -3.40862264, -3.40862264,\n",
       "       -3.04056207, -1.77213495, -2.15934671, -4.34290709, -1.05760951,\n",
       "       -3.88949519, -1.34876792, -1.45467241, -1.45467241, -1.05760951,\n",
       "       -3.24836749, -1.45467241, -2.15934671, -4.62434577, -2.15934671,\n",
       "       -1.05760951, -3.40862264, -3.40862264, -3.04056207, -1.05760951,\n",
       "       -3.04056207, -3.40862264, -3.40862264, -2.15934671, -2.15934671,\n",
       "       -0.53029923, -3.24836749, -3.40862264, -1.05760951, -2.54483324,\n",
       "       -1.05760951, -3.20726203, -3.88949519, -2.54483324, -3.40862264,\n",
       "       -3.12892925, -1.05760951, -3.88949519, -1.45467241, -2.54483324,\n",
       "       -1.05760951, -0.53029923, -2.15934671, -1.05760951, -1.34876792,\n",
       "       -1.34876792, -0.53029923, -3.66030991, -2.15934671, -1.05760951,\n",
       "       -2.54483324, -2.54483324, -1.05760951, -0.53029923, -2.15934671,\n",
       "       -0.53029923, -0.53029923, -2.66653865, -2.15934671, -3.40862264,\n",
       "       -2.54483324, -2.15934671, -2.54483324, -3.40862264, -0.53029923,\n",
       "       -1.45467241, -2.15934671, -2.15934671, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951, -1.05760951, -1.05760951, -1.05760951, -1.05760951,\n",
       "       -1.05760951])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.predict_log_proba(data_tfidf_test.iloc[:,:(data_tfidf_train.shape[1]-1)])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8283261802575107\n"
     ]
    }
   ],
   "source": [
    "label_10 = df_train[\"label\"]==5\n",
    "label_10_test = df_test[\"label\"]==5\n",
    "\n",
    "mr = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:len(gen_des)],label_10)\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,:len(gen_des)])\n",
    "\n",
    "print(sum(pred == label_10_test)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540772532188841\n",
      "4.954935622317596\n"
     ]
    }
   ],
   "source": [
    "mr = LogisticRegression(penalty='l1',solver='liblinear').fit(data_tfidf_train.iloc[:,:(data_tfidf_train.shape[1]-1)],np.array(df_train[\"label\"]))\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,:(data_tfidf_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == df_test[\"label\"])/len(pred))\n",
    "print(np.mean((pred-df_test[\"label\"])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3540772532188841\n",
      "4.954935622317596\n"
     ]
    }
   ],
   "source": [
    "mr = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,:len(gen_des)],np.array(df_train[\"label\"]))\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,:len(gen_des)])\n",
    "\n",
    "print(sum(pred == df_test[\"label\"])/len(pred))\n",
    "print(np.mean((pred-df_test[\"label\"])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17381974248927037\n",
      "9.17596566523605\n"
     ]
    }
   ],
   "source": [
    "mr = LogisticRegression(penalty='l2',dual=True,solver=\"liblinear\").fit(data_tfidf_train.iloc[:,len(gen_des):(data_tfidf_train.shape[1]-1)],np.array(df_train[\"label\"]))\n",
    "pred = mr.predict(data_tfidf_test.iloc[:,len(gen_des):(data_tfidf_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == df_test[\"label\"])/len(pred))\n",
    "print(np.mean((pred-df_test[\"label\"])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39914163090128757\n",
      "2.847639484978541\n"
     ]
    }
   ],
   "source": [
    "mr = LogisticRegression(penalty='l1',solver=\"liblinear\").fit(data_train.iloc[:,:(data_train.shape[1]-1)],np.array(df_train[\"label\"]))\n",
    "pred = mr.predict(data_test.iloc[:,:(data_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == df_test[\"label\"])/len(pred))\n",
    "print(np.mean((pred-df_test[\"label\"])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17381974248927037\n",
      "9.169527896995708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(with_mean=False),SVC(gamma='auto'))\n",
    "clf.fit(data_train.iloc[:,:(data_train.shape[1]-1)],np.array(df_train[\"label\"]))\n",
    "pred = clf.predict(data_test.iloc[:,:(data_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == df_test[\"label\"])/len(pred))\n",
    "print(np.mean((pred-df_test[\"label\"])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38412017167381973\n",
      "2.944206008583691\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(data_test.iloc[:,:(data_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == df_test[\"label\"])/len(pred))\n",
    "print(np.mean((pred-df_test[\"label\"])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.9715226190092192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "krr = KernelRidge(kernel=\"rbf\")\n",
    "krr.fit(data_train.iloc[:,:(data_train.shape[1]-1)],np.array(df_train[\"label\"]))\n",
    "pred = krr.predict(data_test.iloc[:,:(data_test.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == df_test[\"label\"])/len(pred))\n",
    "print(np.mean((pred-df_test[\"label\"])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.58669991e-01,  6.33187301e-01,  1.25089401e+00,  6.05644719e-01,\n",
       "        2.85934843e+00,  7.29419154e-01,  1.50625893e+00,  9.76500843e-01,\n",
       "        2.39299635e-01,  2.19459650e+00,  9.45028864e-01,  4.99883965e-01,\n",
       "        1.41371816e+00,  2.18710075e+00,  8.19708537e-01,  6.58455754e-01,\n",
       "        2.04044767e+00,  8.99073823e-01,  3.61597019e-01,  1.61709703e+00,\n",
       "        9.51757965e-01,  1.28238660e+00,  2.87098267e-01,  2.98056482e+00,\n",
       "        1.25377523e+00,  7.61212906e-01,  4.61243924e-01,  2.44101023e+00,\n",
       "        1.39601197e+00,  1.61628588e+00,  8.68157277e-01,  8.21974898e-01,\n",
       "        6.49968742e-01,  1.16604946e+00,  1.07099592e+00,  9.73266820e-01,\n",
       "        1.56799796e+00,  2.14615292e+00,  1.13195589e+00,  8.44978386e-01,\n",
       "        1.79716250e+00,  1.77871976e+00,  9.02608821e-01,  9.80913511e-01,\n",
       "        1.56471308e+00,  8.95150682e-01,  1.86381207e+00,  6.11169134e-01,\n",
       "        2.00341177e+00,  1.21429975e+00,  1.58086962e+00,  7.51002094e-01,\n",
       "        2.11713234e+00,  8.23875159e-01,  2.20354632e+00,  1.38659398e+00,\n",
       "        2.22758895e+00,  4.49502477e-01,  7.81171182e-01,  2.07817125e+00,\n",
       "        2.40687651e+00,  1.12109539e+00,  2.40254454e+00,  1.06588042e+00,\n",
       "        2.41558621e+00,  5.49957892e-01,  9.32256331e-01,  1.78189680e+00,\n",
       "        2.61874109e+00,  2.44272714e+00,  2.51546726e+00,  3.91878351e-01,\n",
       "        1.64837021e+00,  1.85082686e-01,  3.67512324e+00,  1.38454898e+00,\n",
       "        8.96836669e-02,  2.42658537e+00,  1.14190133e+00,  3.04957209e+00,\n",
       "        9.04370772e-01,  2.34186475e+00,  6.13224307e-01,  4.90344826e-01,\n",
       "        9.74742304e-01,  1.91407497e+00,  1.91528864e+00,  1.21887941e+00,\n",
       "        2.08631855e+00,  1.09729110e+00,  1.63641180e+00,  1.47234569e+00,\n",
       "        8.71414033e-01,  1.85214834e+00,  1.33611217e+00,  2.53041164e+00,\n",
       "        1.69663722e+00,  6.32970862e-01,  1.07489840e+00,  2.49326946e+00,\n",
       "        1.84359834e+00,  2.68250284e+00,  1.25971914e+00,  8.55605111e-01,\n",
       "        1.18934251e+00,  1.05291087e+00,  1.57422436e+00,  2.83299332e+00,\n",
       "        2.31766438e+00,  2.96633815e+00,  2.57615400e+00,  9.40794681e-01,\n",
       "        9.94411359e-01,  1.87111338e+00,  7.66541240e-01,  1.02531338e+00,\n",
       "        7.84505619e-01,  1.49044197e+00,  3.02639996e+00,  1.36996283e+00,\n",
       "        2.18437801e+00,  5.72206436e-01,  2.04047160e+00,  1.97159048e+00,\n",
       "        2.91880193e+00,  1.79047743e+00,  1.01769255e+00,  2.98211318e+00,\n",
       "        1.52398189e+00,  1.45044418e+00,  1.51870665e+00,  1.24719843e+00,\n",
       "        2.19885387e+00,  1.26169431e+00,  1.78628612e+00,  9.47615292e-01,\n",
       "        3.80161381e-01,  1.93080026e+00,  7.85654504e-01,  1.01746727e+00,\n",
       "        1.81958360e+00,  1.05699557e+00,  1.09677494e+00,  1.96413520e+00,\n",
       "        2.71863676e+00,  1.87606113e+00, -1.24530985e-89,  1.00060878e+00,\n",
       "        1.73901760e+00,  1.22319460e+00,  2.51774641e+00,  2.07790599e+00,\n",
       "        8.17478876e-01,  1.47689351e+00,  1.56121795e+00,  1.99914025e+00,\n",
       "        2.58658065e+00,  1.87118501e+00,  1.50907243e+00,  1.58619930e+00,\n",
       "        9.26193315e-01,  2.30008670e+00,  1.88422114e+00,  2.84004655e+00,\n",
       "        2.62112027e+00,  2.43403273e+00,  2.57979257e+00,  1.99220912e+00,\n",
       "        3.75934979e+00,  1.99724446e+00,  2.22533016e+00,  1.22496919e+00,\n",
       "        1.75862074e+00,  2.20204880e+00,  2.54544419e+00,  1.51022410e+00,\n",
       "        1.48757855e+00,  1.46966441e+00,  2.58582119e+00,  2.00837775e+00,\n",
       "        1.85447323e+00,  1.63390897e+00,  2.15178659e+00,  1.03429254e+00,\n",
       "        1.26311059e+00,  1.50197231e+00,  2.41533829e+00,  4.08493839e+00,\n",
       "        9.85867494e-01,  3.96969215e+00,  2.32077643e+00,  7.99443467e-01,\n",
       "        1.40041481e+00,  4.14961335e-01,  1.82918403e+00,  2.39500865e+00,\n",
       "        2.12000486e+00,  3.42579353e+00,  2.84173527e+00,  3.28421805e+00,\n",
       "        2.01906131e+00,  1.49272282e+00,  2.39809142e+00,  1.16549036e-18,\n",
       "        2.19708225e+00,  2.01090691e+00,  1.12275841e+00,  2.16299214e+00,\n",
       "        2.69070422e+00,  3.07449917e+00,  1.76694267e+00,  1.34385370e+00,\n",
       "        1.87099769e+00,  3.45192377e+00,  1.46410550e+00,  3.11178342e+00,\n",
       "        1.69432719e+00,  2.35433090e+00,  2.62608694e+00,  3.24223356e+00,\n",
       "        3.17306549e+00,  1.96730455e+00,  9.48324629e-01,  2.26671093e+00,\n",
       "        2.83936033e+00,  1.22508489e+00,  2.56396807e+00,  2.65488316e+00,\n",
       "        1.46262976e+00,  2.22371923e+00,  3.21557127e+00,  2.41316018e+00,\n",
       "        2.61270884e+00,  1.62668340e+00,  2.04831294e+00,  1.34293505e+00,\n",
       "        3.75890076e+00,  1.36532496e+00,  3.36131679e+00,  3.57156420e+00,\n",
       "        1.65161119e+00,  2.65955207e+00,  1.53332775e+00,  9.72405074e-01,\n",
       "        2.00243560e+00,  2.52568543e+00,  1.00446722e+00,  4.11627369e+00,\n",
       "        2.55989252e+00,  8.26058509e-01,  1.45538309e+00,  3.29020938e+00,\n",
       "        1.95250494e+00,  2.10902342e+00,  2.25965390e+00,  1.24349825e+00,\n",
       "        2.52733346e+00,  2.47583978e+00,  3.72902311e+00,  3.29804157e+00,\n",
       "        3.06227097e+00,  2.07657032e+00,  3.50249389e+00,  2.35391443e+00,\n",
       "        1.98242614e+00,  2.41377996e+00,  2.69402558e+00,  2.62674439e-01,\n",
       "        2.25344687e+00,  1.51198265e+00,  2.60900938e+00,  2.49131919e+00,\n",
       "        2.35157823e+00,  3.50184843e+00,  3.64589304e+00,  2.53911378e+00,\n",
       "        2.73326659e+00,  2.64777327e+00,  1.34856591e+00,  1.61663089e+00,\n",
       "        1.97179498e+00,  2.35425740e+00,  3.02923280e+00,  3.11172078e+00,\n",
       "        3.30344603e+00,  2.19955940e+00,  2.51500049e+00,  3.34859422e+00,\n",
       "        1.63987112e+00,  2.58824795e+00,  3.64401921e+00,  3.27154806e+00,\n",
       "        3.12031536e+00,  2.01474385e+00,  2.59066301e+00,  3.10410769e+00,\n",
       "        3.92235211e+00,  3.78867638e+00,  2.35870349e+00,  3.95451229e-01,\n",
       "        4.03537635e+00,  2.31927896e+00,  3.36166549e+00,  2.34388313e+00,\n",
       "        1.57108609e+00,  1.38816158e+00,  4.21296168e+00,  1.94592487e+00,\n",
       "        3.48050006e+00,  2.02800447e+00,  2.50810356e+00,  4.66911376e+00,\n",
       "        3.34127019e+00,  3.04901617e+00,  3.26964935e+00,  2.40716914e+00,\n",
       "        2.80501431e+00,  2.18310897e+00,  3.57986813e+00,  2.82118604e+00,\n",
       "        4.20122007e+00,  2.92655359e+00,  3.72235872e+00,  2.77198424e+00,\n",
       "        2.74607748e+00,  3.61644777e+00,  3.36194626e+00,  2.95330627e+00,\n",
       "        5.02278243e+00,  2.00837187e+00,  4.98556269e+00,  5.75794729e+00,\n",
       "        3.17443411e+00,  3.94314990e+00,  2.65449093e+00,  2.47387072e+00,\n",
       "        3.45064720e+00,  3.43243334e+00,  2.72740805e+00,  3.17153870e+00,\n",
       "        2.47768110e+00,  4.18208741e+00,  4.03025537e+00,  3.05044730e+00,\n",
       "        2.87118820e+00,  5.23235979e+00,  2.61467303e+00,  2.94510039e+00,\n",
       "        2.15176610e+00,  2.37551078e-01,  2.83110365e+00,  4.21495103e+00,\n",
       "        3.98208119e+00,  2.31438155e-01,  1.09255121e+00,  1.82848236e+00,\n",
       "        5.52723628e+00,  1.82146734e+00,  1.74374040e+00,  2.29751882e+00,\n",
       "        3.23322901e+00,  2.47701570e+00,  2.41180837e+00,  2.52117675e+00,\n",
       "        3.16160516e+00,  2.11511959e+00,  3.05374414e+00,  2.81340586e+00,\n",
       "        4.09126994e+00,  3.25826563e+00,  5.04660083e+00,  4.74580651e+00,\n",
       "        8.27967107e-01,  2.90142478e+00,  4.86440316e+00,  4.20565389e+00,\n",
       "        4.98830909e+00,  1.39028759e+00,  3.98802422e+00,  2.25758643e+00,\n",
       "        2.13160061e+00,  3.48936961e+00,  4.95426560e+00,  4.06763269e+00,\n",
       "        2.09515437e+00,  3.20571272e+00,  2.39623478e+00,  4.18610984e+00,\n",
       "        3.56593775e+00,  3.78946368e+00,  2.87561726e+00,  3.19236588e+00,\n",
       "        4.64206829e+00,  2.45966812e+00,  2.78035735e+00,  2.69402217e+00,\n",
       "        3.33336311e+00,  2.51772444e+00,  2.19661836e+00,  2.62707521e+00,\n",
       "        3.24270538e+00,  2.37537590e+00,  2.47453275e+00,  4.71273301e-01,\n",
       "        2.64459361e+00,  2.51507504e+00,  1.52233447e+00,  3.17007239e+00,\n",
       "        3.69021741e+00,  2.74787819e+00,  3.57934519e+00,  3.23758103e+00,\n",
       "        3.94809858e+00,  2.76822319e+00,  3.37513761e+00,  3.22318318e+00,\n",
       "        5.80302427e+00,  2.83378740e+00,  2.68664801e+00,  2.79470805e+00,\n",
       "        3.51924281e+00,  1.60216171e+00,  3.56115851e+00,  2.48455300e+00,\n",
       "        2.64653060e+00,  2.37212000e+00,  1.74072754e+00,  3.39342114e+00,\n",
       "        2.54158006e+00,  3.84536247e+00,  1.09349672e+00,  2.81114730e+00,\n",
       "        2.26732223e+00,  4.04389538e+00,  2.52597767e+00,  4.21663342e+00,\n",
       "        3.28479598e+00,  4.47286640e+00,  3.18423843e+00,  3.67075225e+00,\n",
       "        4.69997953e+00,  3.32952568e+00,  3.15671272e+00,  4.07953407e+00,\n",
       "        4.06343545e+00,  3.92875494e+00,  4.33373695e+00,  3.86691412e+00,\n",
       "        5.50001704e+00,  3.81382800e+00,  9.37155822e-01,  2.15013418e+00,\n",
       "        2.26424379e+00,  2.67210482e+00,  3.96082347e+00,  3.30017220e+00,\n",
       "        5.03249420e+00,  2.40491435e+00,  3.24548715e+00,  1.58075870e+00,\n",
       "        3.47022197e+00,  2.30169023e+00,  3.29335923e+00,  3.85771124e+00,\n",
       "        3.41611685e+00,  4.06600455e+00])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
