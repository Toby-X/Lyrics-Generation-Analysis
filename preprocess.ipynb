{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "from gensim import models\n",
    "from scipy.sparse import lil_matrix, hstack, csr_matrix, vstack\n",
    "import gensim.downloader as api\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "In this section, we preprocess the data and transform raw text data to matrix form. Then, all data is divided into training set and test set. After that, a dictionary is built upon training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_7796\\3642662710.py:47: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  df.loc[i,\"active_years\"] = int(ay_tmp)\n"
     ]
    }
   ],
   "source": [
    "def specific_preprocess(doc):\n",
    "    return simple_preprocess(doc,min_len=2)\n",
    "\n",
    "def remove_specific_words(s):\n",
    "    s = re.sub(r\"\\bLyrics\",\" \",s)\n",
    "    s = re.sub(r\"\\[.+\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\b\\d+\\b Contributors\",\" \",s)\n",
    "    s = re.sub(r\"Embed\",\" \",s)\n",
    "    s = re.sub(r\"You might also like\",\" \",s)\n",
    "    return s\n",
    "\n",
    "def count_space(s):\n",
    "    return s.count(' ')\n",
    "\n",
    "def remove_short_words(s):\n",
    "    s = re.sub(r\"\\b..\\b\",\" \",s)\n",
    "    s = re.sub(r\"\\b . \\b\",\" \",s)\n",
    "    pronoun = [r\"\\b you\\b\",r\"\\b yours\\b\",r\"\\b him \\b\",r\"\\b his\\b\", r\"\\b she \\b\", r\"\\b her \\b\", r\"\\b hers\\b\",\n",
    "               r\"\\b they \\b\", r\"\\b them \\b\", r\"\\b their \\b\", r\"\\b theirs \\b\",r\"\\b You\\b\",r\"\\b Yours\\b\",\n",
    "               r\"\\b Him \\b\",r\"\\b His\\b\", r\"\\b She \\b\", r\"\\b Her \\b\", r\"\\b Hers\\b\",\n",
    "               r\"\\b They \\b\", r\"\\b Them \\b\", r\"\\b Their \\b\", r\"\\b Theirs \\b\"]\n",
    "    conj = [r\"\\b and \\b\", r\"\\b then \\b\",r\"\\b for\\b\", r\"\\b from\\b\", r\"\\b with\\b\",\n",
    "            r\"\\b about\\b\",r\"\\b And \\b\", r\"\\b Then \\b\",r\"\\b For\\b\", r\"\\b From\\b\", r\"\\b With\\b\",\n",
    "            r\"\\b About\\b\"]\n",
    "    for word in pronoun:\n",
    "        s = re.sub(word,\" \",s)\n",
    "    for word in conj:\n",
    "        s = re.sub(word,\" \",s)\n",
    "    return s\n",
    "\n",
    "def count_lines(s):\n",
    "    res = len(re.findall(r\"\\r\\n\",s))\n",
    "    return res\n",
    "\n",
    "def count_paras(s):\n",
    "    res = len(re.findall(r\"\\r\\n\\r\\n\",s))\n",
    "    return res\n",
    "\n",
    "df = pd.read_csv(\"data/billboard_lyrics_genres.csv\")\n",
    "df_activeyear = pd.read_csv(\"data/first_active_years.csv\")\n",
    "df_activeyear = df_activeyear.drop_duplicates(subset=[\"band_singer\",\"title\",\"year\"],ignore_index=True)\n",
    "df[\"active_years\"] = 0\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    ay_tmp = df_activeyear.loc[(df_activeyear[\"band_singer\"]==df.loc[i,\"band_singer\"])&(df_activeyear[\"title\"]==df.loc[i,\"title\"])&(df_activeyear[\"year\"]==df.loc[i,\"year\"])].active_years\n",
    "    if not ay_tmp.empty:\n",
    "        df.loc[i,\"active_years\"] = int(ay_tmp)\n",
    "\n",
    "df_tmp = df.loc[df[\"active_years\"]!=0].reset_index(drop=True)\n",
    "\n",
    "df[\"numword\"] = df[\"lyrics\"].map(count_space)\n",
    "df[\"num_lines\"] = df[\"lyrics\"].map(count_lines)+1\n",
    "df[\"num_paras\"] = df[\"lyrics\"].map(count_paras)+1\n",
    "df[\"av_word_line\"] = df[\"numword\"]/df[\"num_lines\"]\n",
    "df[\"av_word_paras\"] = df[\"numword\"]/df[\"num_paras\"]\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(remove_specific_words)\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(remove_stopwords)\n",
    "# df[\"lyrics\"] = df[\"lyrics\"].map(remove_short_words)\n",
    "# df.to_csv(\"data/df_cluster.csv\")\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(specific_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_word(L):\n",
    "    unique_words = []\n",
    "    for word in L:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "    return len(unique_words)\n",
    "\n",
    "def find_max_len(L):\n",
    "    max_len = list(map(len,L))\n",
    "    return max(max_len)\n",
    "\n",
    "df[\"unique_words\"] = df[\"lyrics\"].map(find_unique_word)\n",
    "df[\"max_len\"] = df[\"lyrics\"].map(find_max_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the songs that are not English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(w):\n",
    "    return w.encode(\"utf-8\").isalpha()\n",
    "\n",
    "def isListEnglish(L):\n",
    "    return all(map(isEnglish,L))\n",
    "\n",
    "df[\"isEnglish\"] = df[\"lyrics\"].map(isListEnglish)\n",
    "df = df.loc[df[\"isEnglish\"],:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same procedure to genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pun(s):\n",
    "    s = re.sub(r\"\\[\\'\",\" \",s)\n",
    "    s = re.sub(r\"\\'\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\'\",\" \",s)\n",
    "    s = re.sub(r\"\\[\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\,\",\" \",s)\n",
    "    s = s.split()\n",
    "    s = [token.lower() for token in s]\n",
    "    return s\n",
    "\n",
    "\n",
    "df[\"genre\"] = df[\"genre\"].map(remove_pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alternative': 145,\n",
      " 'and': 157,\n",
      " 'country': 416,\n",
      " 'dance-pop': 155,\n",
      " 'disco': 148,\n",
      " 'folk': 141,\n",
      " 'funk': 170,\n",
      " 'hard': 102,\n",
      " 'hip': 435,\n",
      " 'hop': 376,\n",
      " 'new': 168,\n",
      " 'pop': 1383,\n",
      " 'r&b': 665,\n",
      " 'rap': 116,\n",
      " 'rock': 1607,\n",
      " 'roll': 111,\n",
      " 'soft': 322,\n",
      " 'soul': 476,\n",
      " 'wave': 109}\n"
     ]
    }
   ],
   "source": [
    "freq_gen = defaultdict(int)\n",
    "for text in df[\"genre\"]:\n",
    "    for token in text:\n",
    "        freq_gen[token] += 1\n",
    "\n",
    "processed_corpus_gen = [[token for token in text if freq_gen[token]>20] for text in df.loc[:,\"genre\"]]\n",
    "dict_gen = corpora.Dictionary(processed_corpus_gen)\n",
    "freq_wanted = {k: v for k,v in freq_gen.items() if v > 100}\n",
    "pprint.pprint(freq_wanted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we can sort out the genre we want is alternative, country, dance, disco, folk, funk, hip-hop, new wave, pop, r&b, rap, rock, soul (soft stands for soft rock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_des = [\"alternative\",\"country\",\"dance\",\"disco\",\"folk\",\"funk\",\"hip\",\"new\",\"pop\",\"r&b\",\"rap\",\"rock\",\"soul\"]\n",
    "gen_des = sorted(gen_des)\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = len(gen_des)\n",
    "dat_gen = lil_matrix((len(df), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    for word in row[\"genre\"]:\n",
    "        for k in range(len(gen_des)):\n",
    "            if re.search(gen_des[k],word):\n",
    "                dat_gen[i,k] = 1\n",
    "df[df[\"genre\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_gen = pd.DataFrame.sparse.from_spmatrix(dat_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we should tag the data for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = np.zeros(df.shape[0])\n",
    "\n",
    "bins = [1970,1980,1990,2000,2010,np.inf]\n",
    "\n",
    "labels = [0,1,2,3,4,5]\n",
    "\n",
    "df[\"label\"] = np.where(df[\"year\"] < bins[0], labels[0],\n",
    "                               np.where(df[\"year\"] < bins[1], labels[1],\n",
    "                                        np.where(df[\"year\"] < bins[2], labels[2],\n",
    "                                                 np.where(df[\"year\"] < bins[3], labels[3],\n",
    "                                                          np.where(df[\"year\"] < bins[4], labels[4], labels[5])))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, data is split to training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(515)\n",
    "idx = np.repeat(range(10),len(df.iloc[:,0])//10+1)\n",
    "df[\"idx\"] = np.random.choice(idx[range(len(df.iloc[:,0]))],size=len(df.iloc[:,0]))\n",
    "df_train = df.loc[df[\"idx\"]!=0,:]\n",
    "df_test = df.loc[df[\"idx\"]==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[844, 778, 792, 731, 725, 789]\n",
      "[81, 85, 69, 78, 68, 85]\n"
     ]
    }
   ],
   "source": [
    "num_train_dec = []\n",
    "num_test_dec = []\n",
    "for i in range(6):\n",
    "    num_train_dec.append(np.sum(df_train[\"label\"]==i))\n",
    "    num_test_dec.append(np.sum(df_test[\"label\"]==i))\n",
    "\n",
    "print(num_train_dec)\n",
    "print(num_test_dec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dictionary based on training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_392\\4121515207.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n"
     ]
    }
   ],
   "source": [
    "freq = defaultdict(int)\n",
    "for text in df_train[\"lyrics\"]:\n",
    "    for token in text:\n",
    "        freq[token] += 1\n",
    "\n",
    "processed_corpus = [[token for token in text if freq[token]>20] for text in df_train.loc[:,\"lyrics\"]]\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "df_train[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_train = lil_matrix((len(df_train), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"freq_count\"]]\n",
    "    values = [value for _, value in row[\"freq_count\"]]\n",
    "    dat_train[i, indices] = values\n",
    "df_train[df_train[\"freq_count\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_train = pd.DataFrame.sparse.from_spmatrix(dat_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, perform the same procedure to test set with the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_392\\2217317512.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n"
     ]
    }
   ],
   "source": [
    "df_test = df.loc[df[\"idx\"]==0,:]\n",
    "processed_corpus = [[token for token in text if freq[token]>20] for text in df_test.loc[:,\"lyrics\"]]\n",
    "df_test[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_test = lil_matrix((len(df_test), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"freq_count\"] if count < num_cols]\n",
    "    values = [value for count, value in row[\"freq_count\"] if count < num_cols and value!=0]\n",
    "    dat_test[i, indices] = values\n",
    "df_test[df_test[\"freq_count\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_test = pd.DataFrame.sparse.from_spmatrix(dat_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_392\\1318062561.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"tfidf\"]=tfidf[df_train[\"lyrics\"].map(dictionary.doc2bow)]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = list(df_train[\"freq_count\"])\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "df_train[\"tfidf\"]=tfidf[df_train[\"lyrics\"].map(dictionary.doc2bow)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_tfidf_train = lil_matrix((len(df_train), num_cols), dtype=np.float64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"tfidf\"]]\n",
    "    values = [value for _, value in row[\"tfidf\"]]\n",
    "    dat_tfidf_train[i, indices] = values\n",
    "df_train[df_train[\"tfidf\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_tfidf_train = pd.DataFrame.sparse.from_spmatrix(dat_tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_392\\3654324900.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"tfidf\"]=tfidf[df_test[\"lyrics\"].map(dictionary.doc2bow)]\n"
     ]
    }
   ],
   "source": [
    "df_test[\"tfidf\"]=tfidf[df_test[\"lyrics\"].map(dictionary.doc2bow)]\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_tfidf_test = lil_matrix((len(df_test), num_cols), dtype=np.float64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"tfidf\"] if count < num_cols]\n",
    "    values = [value for count, value in row[\"tfidf\"] if count < num_cols and value != 0]\n",
    "    dat_tfidf_test[i, indices] = values\n",
    "df_test[df_test[\"tfidf\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_tfidf_test = pd.DataFrame.sparse.from_spmatrix(dat_tfidf_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed Data\n",
    "The data processed are diveded into the blow categories:\n",
    "\n",
    "Original word frequency + genre\n",
    "\n",
    "TF-IDF word frequency + genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_gen = dat_gen.reset_index()\n",
    "df = df.reset_index(drop=True)\n",
    "dat_gen_train = dat_gen.loc[df[\"idx\"]!=0,:].reset_index(drop=True)\n",
    "dat_gen_test = dat_gen.loc[df[\"idx\"]==0,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ones = csr_matrix(np.ones(df_train.shape[0])).transpose()\n",
    "test_ones = csr_matrix(np.ones(df_test.shape[0])).transpose()\n",
    "train_label = csr_matrix(df_train.loc[:,\"label\"]).transpose()\n",
    "test_label = csr_matrix(df_test.loc[:,\"label\"]).transpose()\n",
    "train_activeyear = csr_matrix(df_train.loc[:,\"active_years\":\"unique_words\"])\n",
    "test_activeyear = csr_matrix(df_test.loc[:,\"active_years\":\"unique_words\"])\n",
    "\n",
    "gen_train = csr_matrix(dat_gen_train.loc[:,0:])\n",
    "lyrics_train = csr_matrix(dat_train.loc[:,0:])\n",
    "data_train = hstack([train_ones,gen_train, lyrics_train,train_activeyear,train_label])\n",
    "data_train = pd.DataFrame.sparse.from_spmatrix(data_train)\n",
    "\n",
    "gen_test = csr_matrix(dat_gen_test.loc[:,0:])\n",
    "lyrics_test = csr_matrix(dat_test.loc[:,0:])\n",
    "data_test = hstack([test_ones,gen_test, lyrics_test,test_activeyear,test_label])\n",
    "data_test = pd.DataFrame.sparse.from_spmatrix(data_test)\n",
    "\n",
    "\n",
    "lyrics_tfidf_train = csr_matrix(dat_tfidf_train.loc[:,0:])\n",
    "data_tfidf_train = hstack([train_ones,gen_train,lyrics_tfidf_train,train_activeyear,train_label])\n",
    "data_tfidf_train = pd.DataFrame.sparse.from_spmatrix(data_tfidf_train)\n",
    "\n",
    "lyrics_tfidf_test = csr_matrix(dat_tfidf_test.loc[:,0:])\n",
    "data_tfidf_test = hstack([test_ones,gen_test,lyrics_tfidf_test,test_activeyear,test_label])\n",
    "data_tfidf_test = pd.DataFrame.sparse.from_spmatrix(data_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_train.loc[:,\"active_years\":\"unique_words\"]\n",
    "word_name = [dictionary[i] for i in range(max(dictionary.keys())+1)]\n",
    "word_name = ['intercept']+gen_des + word_name +list(df_tmp.columns)+ ['label']\n",
    "data_tfidf_test.columns = word_name\n",
    "data_tfidf_train.columns = word_name\n",
    "data_train.columns = word_name\n",
    "data_test.columns = word_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_tfidf_train.to_csv(\"data/train_tfidf_data.csv\")\n",
    "# data_tfidf_test.to_csv(\"data/test_tfidf_data.csv\")\n",
    "# # data_train = hstack([lyrics_train,train_label])\n",
    "# # data_train = pd.DataFrame.sparse.from_spmatrix(data_train)\n",
    "# # data_test = hstack([lyrics_test,test_label])\n",
    "# # data_test = pd.DataFrame.sparse.from_spmatrix(data_test)\n",
    "# # word_name = [dictionary[i] for i in range(max(dictionary.keys())+1)]\n",
    "# # word_name = word_name+['label']\n",
    "# # data_train.columns = word_name\n",
    "# # data_test.columns = word_name\n",
    "\n",
    "# data_train.to_csv(\"data/train_data_all.csv\")\n",
    "# data_test.to_csv(\"data/test_data_all.csv\")\n",
    "# df_train.to_csv(\"data/train_other.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The following section examine different types of 1 on 1 logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "transformer = MaxAbsScaler()\n",
    "transformer.fit(data_train.iloc[:,:(data_train.shape[1]-1)])\n",
    "data_train_scaled = transformer.transform(data_train.iloc[:,:(data_train.shape[1]-1)])\n",
    "data_test_scaled = transformer.transform(data_test.iloc[:,:(data_train.shape[1]-1)])\n",
    "data_train_scaled = hstack([data_train_scaled,train_label])\n",
    "data_test_scaled = hstack([data_test_scaled,test_label])\n",
    "data_train_scaled = pd.DataFrame.sparse.from_spmatrix(data_train_scaled)\n",
    "data_test_scaled = pd.DataFrame.sparse.from_spmatrix(data_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "data_train_scaled_60 = data_train_scaled.loc[data_train_scaled.iloc[:,(data_train_scaled.shape[1]-1)]==0,:]\n",
    "data_train_scaled_70 = data_train_scaled.loc[data_train_scaled.iloc[:,(data_train_scaled.shape[1]-1)]==1,:]\n",
    "data_train_scaled_80 = data_train_scaled.loc[data_train_scaled.iloc[:,(data_train_scaled.shape[1]-1)]==2,:]\n",
    "data_train_scaled_90 = data_train_scaled.loc[data_train_scaled.iloc[:,(data_train_scaled.shape[1]-1)]==3,:]\n",
    "data_train_scaled_00 = data_train_scaled.loc[data_train_scaled.iloc[:,(data_train_scaled.shape[1]-1)]==4,:]\n",
    "data_train_scaled_10 = data_train_scaled.loc[data_train_scaled.iloc[:,(data_train_scaled.shape[1]-1)]==5,:]\n",
    "\n",
    "data_test_scaled_60 = data_test_scaled.loc[data_test_scaled.iloc[:,(data_test_scaled.shape[1]-1)]==0,:]\n",
    "data_test_scaled_70 = data_test_scaled.loc[data_test_scaled.iloc[:,(data_test_scaled.shape[1]-1)]==1,:]\n",
    "data_test_scaled_80 = data_test_scaled.loc[data_test_scaled.iloc[:,(data_test_scaled.shape[1]-1)]==2,:]\n",
    "data_test_scaled_90 = data_test_scaled.loc[data_test_scaled.iloc[:,(data_test_scaled.shape[1]-1)]==3,:]\n",
    "data_test_scaled_00 = data_test_scaled.loc[data_test_scaled.iloc[:,(data_test_scaled.shape[1]-1)]==4,:]\n",
    "data_test_scaled_10 = data_test_scaled.loc[data_test_scaled.iloc[:,(data_test_scaled.shape[1]-1)]==5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.608433734939759\n"
     ]
    }
   ],
   "source": [
    "# 60s and 70s\n",
    "data_train_scaled_67 = pd.concat([data_train_scaled_60,data_train_scaled_70],axis=0)\n",
    "data_test_scaled_67 = pd.concat([data_test_scaled_60,data_test_scaled_70],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_67.iloc[:,:(data_train_scaled_67.shape[1]-1)],data_train_scaled_67.iloc[:,(data_train_scaled_67.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_67.iloc[:,:(data_test_scaled_67.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_67.iloc[:,(data_test_scaled_67.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7533333333333333\n"
     ]
    }
   ],
   "source": [
    "# 60s and 80s\n",
    "data_train_scaled_68 = pd.concat([data_train_scaled_60,data_train_scaled_80],axis=0)\n",
    "data_test_scaled_68 = pd.concat([data_test_scaled_60,data_test_scaled_80],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_68.iloc[:,:(data_train_scaled_68.shape[1]-1)],data_train_scaled_68.iloc[:,(data_train_scaled_68.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_68.iloc[:,:(data_test_scaled_68.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_68.iloc[:,(data_test_scaled_68.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8742138364779874\n"
     ]
    }
   ],
   "source": [
    "# 60s and 90s\n",
    "data_train_scaled_69 = pd.concat([data_train_scaled_60,data_train_scaled_90],axis=0)\n",
    "data_test_scaled_69 = pd.concat([data_test_scaled_60,data_test_scaled_90],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_69.iloc[:,:(data_train_scaled_69.shape[1]-1)],data_train_scaled_69.iloc[:,(data_train_scaled_69.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_69.iloc[:,:(data_test_scaled_69.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_69.iloc[:,(data_test_scaled_69.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8993288590604027\n"
     ]
    }
   ],
   "source": [
    "# 60s and 00s\n",
    "data_train_scaled_600 = pd.concat([data_train_scaled_60,data_train_scaled_00],axis=0)\n",
    "data_test_scaled_600 = pd.concat([data_test_scaled_60,data_test_scaled_00],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_600.iloc[:,:(data_train_scaled_600.shape[1]-1)],data_train_scaled_600.iloc[:,(data_train_scaled_600.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_600.iloc[:,:(data_test_scaled_600.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_600.iloc[:,(data_test_scaled_600.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8855421686746988\n"
     ]
    }
   ],
   "source": [
    "# 60s and 10s\n",
    "data_train_scaled_61 = pd.concat([data_train_scaled_60,data_train_scaled_10],axis=0)\n",
    "data_test_scaled_61 = pd.concat([data_test_scaled_60,data_test_scaled_10],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_61.iloc[:,:(data_train_scaled_61.shape[1]-1)],data_train_scaled_61.iloc[:,(data_train_scaled_61.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_61.iloc[:,:(data_test_scaled_61.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_61.iloc[:,(data_test_scaled_61.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7402597402597403\n"
     ]
    }
   ],
   "source": [
    "# 70s and 80s\n",
    "data_train_scaled_78 = pd.concat([data_train_scaled_70,data_train_scaled_80],axis=0)\n",
    "data_test_scaled_78 = pd.concat([data_test_scaled_70,data_test_scaled_80],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_78.iloc[:,:(data_train_scaled_78.shape[1]-1)],data_train_scaled_78.iloc[:,(data_train_scaled_78.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_78.iloc[:,:(data_test_scaled_78.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_78.iloc[:,(data_test_scaled_78.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8773006134969326\n"
     ]
    }
   ],
   "source": [
    "# 70s and 90s\n",
    "data_train_scaled_79 = pd.concat([data_train_scaled_70,data_train_scaled_90],axis=0)\n",
    "data_test_scaled_79 = pd.concat([data_test_scaled_70,data_test_scaled_90],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_79.iloc[:,:(data_train_scaled_79.shape[1]-1)],data_train_scaled_79.iloc[:,(data_train_scaled_79.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_79.iloc[:,:(data_test_scaled_79.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_79.iloc[:,(data_test_scaled_79.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9084967320261438\n"
     ]
    }
   ],
   "source": [
    "# 70s and 00s\n",
    "data_train_scaled_700 = pd.concat([data_train_scaled_70,data_train_scaled_00],axis=0)\n",
    "data_test_scaled_700 = pd.concat([data_test_scaled_70,data_test_scaled_00],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_700.iloc[:,:(data_train_scaled_700.shape[1]-1)],data_train_scaled_700.iloc[:,(data_train_scaled_700.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_700.iloc[:,:(data_test_scaled_700.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_700.iloc[:,(data_test_scaled_700.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "# 70s and 10s\n",
    "data_train_scaled_71 = pd.concat([data_train_scaled_70,data_train_scaled_10],axis=0)\n",
    "data_test_scaled_71 = pd.concat([data_test_scaled_70,data_test_scaled_10],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_71.iloc[:,:(data_train_scaled_71.shape[1]-1)],data_train_scaled_71.iloc[:,(data_train_scaled_71.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_71.iloc[:,:(data_test_scaled_71.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_71.iloc[:,(data_test_scaled_71.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8435374149659864\n"
     ]
    }
   ],
   "source": [
    "# 80s and 90s\n",
    "data_train_scaled_89 = pd.concat([data_train_scaled_80,data_train_scaled_90],axis=0)\n",
    "data_test_scaled_89 = pd.concat([data_test_scaled_80,data_test_scaled_90],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_89.iloc[:,:(data_train_scaled_89.shape[1]-1)],data_train_scaled_89.iloc[:,(data_train_scaled_89.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_89.iloc[:,:(data_test_scaled_89.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_89.iloc[:,(data_test_scaled_89.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8686131386861314\n"
     ]
    }
   ],
   "source": [
    "# 80s and 00s\n",
    "data_train_scaled_800 = pd.concat([data_train_scaled_80,data_train_scaled_00],axis=0)\n",
    "data_test_scaled_800 = pd.concat([data_test_scaled_80,data_test_scaled_00],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_800.iloc[:,:(data_train_scaled_800.shape[1]-1)],data_train_scaled_800.iloc[:,(data_train_scaled_800.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_800.iloc[:,:(data_test_scaled_800.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_800.iloc[:,(data_test_scaled_800.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8766233766233766\n"
     ]
    }
   ],
   "source": [
    "# 80s and 10s\n",
    "data_train_scaled_81 = pd.concat([data_train_scaled_80,data_train_scaled_10],axis=0)\n",
    "data_test_scaled_81 = pd.concat([data_test_scaled_80,data_test_scaled_10],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_81.iloc[:,:(data_train_scaled_81.shape[1]-1)],data_train_scaled_81.iloc[:,(data_train_scaled_81.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_81.iloc[:,:(data_test_scaled_81.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_81.iloc[:,(data_test_scaled_81.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7465753424657534\n"
     ]
    }
   ],
   "source": [
    "# 90s and 00s\n",
    "data_train_scaled_900 = pd.concat([data_train_scaled_90,data_train_scaled_00],axis=0)\n",
    "data_test_scaled_900 = pd.concat([data_test_scaled_90,data_test_scaled_00],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_900.iloc[:,:(data_train_scaled_900.shape[1]-1)],data_train_scaled_900.iloc[:,(data_train_scaled_900.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_900.iloc[:,:(data_test_scaled_900.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_900.iloc[:,(data_test_scaled_900.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8220858895705522\n"
     ]
    }
   ],
   "source": [
    "# 90s and 10s\n",
    "data_train_scaled_91 = pd.concat([data_train_scaled_90,data_train_scaled_10],axis=0)\n",
    "data_test_scaled_91 = pd.concat([data_test_scaled_90,data_test_scaled_10],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_91.iloc[:,:(data_train_scaled_91.shape[1]-1)],data_train_scaled_91.iloc[:,(data_train_scaled_91.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_91.iloc[:,:(data_test_scaled_91.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_91.iloc[:,(data_test_scaled_91.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7712418300653595\n"
     ]
    }
   ],
   "source": [
    "# 00s and 10s\n",
    "data_train_scaled_01 = pd.concat([data_train_scaled_00,data_train_scaled_10],axis=0)\n",
    "data_test_scaled_01 = pd.concat([data_test_scaled_00,data_test_scaled_10],axis=0)\n",
    "mr = LogisticRegression(penalty=\"l2\",solver=\"liblinear\")\n",
    "mr.fit(data_train_scaled_01.iloc[:,:(data_train_scaled_01.shape[1]-1)],data_train_scaled_01.iloc[:,(data_train_scaled_01.shape[1]-1)])\n",
    "\n",
    "pred = mr.predict(data_test_scaled_01.iloc[:,:(data_test_scaled_01.shape[1]-1)])\n",
    "print(sum(pred == data_test_scaled_01.iloc[:,(data_test_scaled_01.shape[1]-1)])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "div_mat = np.zeros((6,6))\n",
    "div_mat[0,1] = 0.6084\n",
    "div_mat[0,2] = 0.7533\n",
    "div_mat[0,3] = 0.8742\n",
    "div_mat[0,4] = 0.8993\n",
    "div_mat[0,5] = 0.8855\n",
    "div_mat[1,2] = 0.7403\n",
    "div_mat[1,3] = 0.8773\n",
    "div_mat[1,4] = 0.9085\n",
    "div_mat[1,5] = 0.9\n",
    "div_mat[2,3] = 0.8435\n",
    "div_mat[2,4] = 0.8686\n",
    "div_mat[2,5] = 0.8766\n",
    "div_mat[3,4] = 0.7466\n",
    "div_mat[3,5] = 0.8221\n",
    "div_mat[4,5] = 0.7712\n",
    "for i in range(6):\n",
    "    for j in range(i):\n",
    "        div_mat[i,j] = div_mat[j,i]\n",
    "\n",
    "div_mat = pd.DataFrame(div_mat)\n",
    "div_mat.columns = [\"60s\",\"70s\",\"80s\",\"90s\",\"00s\",\"10s\"]\n",
    "div_mat.index = [\"60s\",\"70s\",\"80s\",\"90s\",\"00s\",\"10s\"]\n",
    "\n",
    "sns.heatmap(div_mat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, it seems that 60s, 70s and 80s are more similar compared with other decades. The following section focuses on combining the three decades"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with $l_1$ penalty, $l_2$ is better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize and Bigram\n",
    "Perform Lemmatize and bigram to lyrics and perform logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(L):\n",
    "    res = list(map(lemmatizer.lemmatize,L))\n",
    "    return res\n",
    "df[\"lyrics\"]=df[\"lyrics\"].map(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df[\"lyrics\"])\n",
    "bigram = Phrases(docs,min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dict_lda = Dictionary(docs)\n",
    "# dict_lda.filter_extremes(no_below=20,no_above=.5)\n",
    "dict_lda.filter_extremes(no_below=20)\n",
    "\n",
    "corpus = [dict_lda.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 2647\n",
      "Number of documents: 5125\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dict_lda))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
