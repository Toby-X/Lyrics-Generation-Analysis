{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "from gensim import models\n",
    "from scipy.sparse import lil_matrix, hstack, csr_matrix, vstack\n",
    "import gensim.downloader as api\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_21500\\2572825781.py:38: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  df.loc[i,\"active_years\"] = int(ay_tmp)\n"
     ]
    }
   ],
   "source": [
    "def specific_preprocess(doc):\n",
    "    return simple_preprocess(doc,min_len=1)\n",
    "\n",
    "def remove_specific_words(s):\n",
    "    s = re.sub(r\"\\bLyrics\",\" \",s)\n",
    "    s = re.sub(r\"\\[.+\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\b\\d+\\b Contributors\",\" \",s)\n",
    "    s = re.sub(r\"Embed\",\" \",s)\n",
    "    return s\n",
    "\n",
    "def count_space(s):\n",
    "    return s.count(' ')\n",
    "\n",
    "def remove_short_words(s):\n",
    "    s = re.sub(r\"\\b..\\b\",\" \",s)\n",
    "    s = re.sub(r\"\\b . \\b\",\" \",s)\n",
    "    pronoun = [r\"\\b you\\b\",r\"\\b yours\\b\",r\"\\b him \\b\",r\"\\b his\\b\", r\"\\b she \\b\", r\"\\b her \\b\", r\"\\b hers\\b\",\n",
    "               r\"\\b they \\b\", r\"\\b them \\b\", r\"\\b their \\b\", r\"\\b theirs \\b\",r\"\\b You\\b\",r\"\\b Yours\\b\",\n",
    "               r\"\\b Him \\b\",r\"\\b His\\b\", r\"\\b She \\b\", r\"\\b Her \\b\", r\"\\b Hers\\b\",\n",
    "               r\"\\b They \\b\", r\"\\b Them \\b\", r\"\\b Their \\b\", r\"\\b Theirs \\b\"]\n",
    "    conj = [r\"\\b and \\b\", r\"\\b then \\b\",r\"\\b for\\b\", r\"\\b from\\b\", r\"\\b with\\b\",\n",
    "            r\"\\b about\\b\",r\"\\b And \\b\", r\"\\b Then \\b\",r\"\\b For\\b\", r\"\\b From\\b\", r\"\\b With\\b\",\n",
    "            r\"\\b About\\b\"]\n",
    "    for word in pronoun:\n",
    "        s = re.sub(word,\" \",s)\n",
    "    for word in conj:\n",
    "        s = re.sub(word,\" \",s)\n",
    "    return s\n",
    "\n",
    "df = pd.read_csv(\"data/billboard_lyrics_genres.csv\")\n",
    "df_activeyear = pd.read_csv(\"data/first_active_years.csv\")\n",
    "df_activeyear = df_activeyear.drop_duplicates(subset=[\"band_singer\",\"title\",\"year\"],ignore_index=True)\n",
    "df[\"active_years\"] = 0\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    ay_tmp = df_activeyear.loc[(df_activeyear[\"band_singer\"]==df.loc[i,\"band_singer\"])&(df_activeyear[\"title\"]==df.loc[i,\"title\"])&(df_activeyear[\"year\"]==df.loc[i,\"year\"])].active_years\n",
    "    if not ay_tmp.empty:\n",
    "        df.loc[i,\"active_years\"] = int(ay_tmp)\n",
    "\n",
    "df_tmp = df.loc[df[\"active_years\"]!=0].reset_index(drop=True)\n",
    "\n",
    "df[\"numword\"] = df[\"lyrics\"].map(count_space)\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(remove_specific_words)\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(remove_stopwords)\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(specific_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(w):\n",
    "    return w.encode(\"utf-8\").isalpha()\n",
    "\n",
    "def isListEnglish(L):\n",
    "    return all(map(isEnglish,L))\n",
    "\n",
    "df[\"isEnglish\"] = df[\"lyrics\"].map(isListEnglish)\n",
    "df = df.loc[df[\"isEnglish\"],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# def lemmatize(L):\n",
    "#     res = list(map(lemmatizer.lemmatize,L))\n",
    "#     return res\n",
    "# df[\"lyrics\"]=df[\"lyrics\"].map(lemmatize)\n",
    "\n",
    "# docs = list(df[\"lyrics\"])\n",
    "# bigram = Phrases(docs,min_count=20)\n",
    "# for idx in range(len(docs)):\n",
    "#     for token in bigram[docs[idx]]:\n",
    "#         if '_' in token:\n",
    "#             docs[idx].append(token)\n",
    "\n",
    "# df[\"lyrics\"] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pun(s):\n",
    "    s = re.sub(r\"\\[\\'\",\" \",s)\n",
    "    s = re.sub(r\"\\'\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\'\",\" \",s)\n",
    "    s = re.sub(r\"\\[\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\,\",\" \",s)\n",
    "    s = s.split()\n",
    "    s = [token.lower() for token in s]\n",
    "    return s\n",
    "\n",
    "\n",
    "df[\"genre\"] = df[\"genre\"].map(remove_pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_des = [\"alternative\",\"country\",\"dance\",\"disco\",\"folk\",\"funk\",\"hip\",\"new\",\"pop\",\"r&b\",\"rap\",\"rock\",\"soul\"]\n",
    "gen_des = sorted(gen_des)\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = len(gen_des)\n",
    "dat_gen = lil_matrix((len(df), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    for word in row[\"genre\"]:\n",
    "        for k in range(len(gen_des)):\n",
    "            if re.search(gen_des[k],word):\n",
    "                dat_gen[i,k] = 1\n",
    "df[df[\"genre\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_gen = pd.DataFrame.sparse.from_spmatrix(dat_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = np.zeros(df.shape[0])\n",
    "\n",
    "bins = [1970,1980,1990,2000,2010,np.inf]\n",
    "\n",
    "labels = [0,1,2,3,4,5]\n",
    "\n",
    "df[\"label\"] = np.where(df[\"year\"] < bins[0], labels[0],\n",
    "                               np.where(df[\"year\"] < bins[1], labels[1],\n",
    "                                        np.where(df[\"year\"] < bins[2], labels[2],\n",
    "                                                 np.where(df[\"year\"] < bins[3], labels[3],\n",
    "                                                          np.where(df[\"year\"] < bins[4], labels[4], labels[5])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = np.zeros(df.shape[0])\n",
    "\n",
    "bins = [1968,1978,1990,np.inf]\n",
    "\n",
    "labels = [0,1,2,3,4]\n",
    "\n",
    "df[\"label\"] = np.where(df[\"year\"] < bins[0], labels[0],\n",
    "                               np.where(df[\"year\"] < bins[1], labels[1],\n",
    "                                        np.where(df[\"year\"] < bins[2], labels[2],\n",
    "                                                 np.where(df[\"year\"] < bins[3], labels[3], labels[4]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(515)\n",
    "idx = np.repeat(range(10),len(df.iloc[:,0])//10+1)\n",
    "df[\"idx\"] = np.random.choice(idx[range(len(df.iloc[:,0]))],size=len(df.iloc[:,0]))\n",
    "df_train = df.loc[df[\"idx\"]!=0,:]\n",
    "df_test = df.loc[df[\"idx\"]==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_21500\\2673658631.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"lyrics\"] = [dict.doc2bow(doc) for doc in df_train[\"lyrics\"]]\n",
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_21500\\2673658631.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"lyrics\"] = [dict.doc2bow(doc) for doc in df_test[\"lyrics\"]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dict = Dictionary(df_train[\"lyrics\"])\n",
    "dict.filter_extremes(no_below=20,no_above=1)\n",
    "\n",
    "df_train[\"lyrics\"] = [dict.doc2bow(doc) for doc in df_train[\"lyrics\"]]\n",
    "df_test[\"lyrics\"] = [dict.doc2bow(doc) for doc in df_test[\"lyrics\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dict.keys())+1\n",
    "dat_train = lil_matrix((len(df_train), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"lyrics\"]]\n",
    "    values = [value for _, value in row[\"lyrics\"]]\n",
    "    dat_train[i, indices] = values\n",
    "df_train[df_train[\"lyrics\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_train = pd.DataFrame.sparse.from_spmatrix(dat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dict.keys())+1\n",
    "dat_test = lil_matrix((len(df_test), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"lyrics\"] if count < num_cols]\n",
    "    values = [value for count, value in row[\"lyrics\"] if count < num_cols and value!=0]\n",
    "    dat_test[i, indices] = values\n",
    "df_test[df_test[\"lyrics\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_test = pd.DataFrame.sparse.from_spmatrix(dat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_gen = dat_gen.reset_index()\n",
    "df = df.reset_index(drop=True)\n",
    "dat_gen_train = dat_gen.loc[df[\"idx\"]!=0,:].reset_index(drop=True)\n",
    "dat_gen_test = dat_gen.loc[df[\"idx\"]==0,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ones = csr_matrix(np.zeros(df_train.shape[0])).transpose()\n",
    "test_ones = csr_matrix(np.zeros(df_test.shape[0])).transpose()\n",
    "train_label = csr_matrix(df_train.loc[:,\"label\"]).transpose()\n",
    "test_label = csr_matrix(df_test.loc[:,\"label\"]).transpose()\n",
    "train_numword = csr_matrix(df_train.loc[:,\"numword\"]).transpose()\n",
    "test_numword = csr_matrix(df_test.loc[:,\"numword\"]).transpose()\n",
    "train_activeyear = csr_matrix(df_train.loc[:,\"active_years\"]).transpose()\n",
    "test_activeyear = csr_matrix(df_test.loc[:,\"active_years\"]).transpose()\n",
    "\n",
    "gen_train = csr_matrix(dat_gen_train.loc[:,0:])\n",
    "lyrics_train = csr_matrix(dat_train.loc[:,0:])\n",
    "data_train = hstack([train_ones,gen_train, lyrics_train,train_numword,train_activeyear,train_label])\n",
    "data_train = pd.DataFrame.sparse.from_spmatrix(data_train)\n",
    "\n",
    "gen_test = csr_matrix(dat_gen_test.loc[:,0:])\n",
    "lyrics_test = csr_matrix(dat_test.loc[:,0:])\n",
    "data_test = hstack([test_ones,gen_test, lyrics_test,test_numword,test_activeyear,test_label])\n",
    "data_test = pd.DataFrame.sparse.from_spmatrix(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "transformer = MaxAbsScaler()\n",
    "transformer.fit(data_train.iloc[:,:(data_train.shape[1]-1)])\n",
    "data_train_scaled = transformer.transform(data_train.iloc[:,:(data_train.shape[1]-1)])\n",
    "data_test_scaled = transformer.transform(data_test.iloc[:,:(data_train.shape[1]-1)])\n",
    "data_train_scaled = hstack([data_train_scaled,train_label])\n",
    "data_test_scaled = hstack([data_test_scaled,test_label])\n",
    "data_train_scaled = pd.DataFrame.sparse.from_spmatrix(data_train_scaled)\n",
    "data_test_scaled = pd.DataFrame.sparse.from_spmatrix(data_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_name = [dict[i] for i in range(max(dict.keys())+1)]\n",
    "word_name = ['intercept']+gen_des + word_name +['numword']+['active_years']+ ['label']\n",
    "data_train.columns = word_name\n",
    "data_test.columns = word_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5579399141630901\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mr = LogisticRegression(penalty='l2',solver=\"liblinear\",max_iter=4000).fit(data_train.iloc[:,:(data_train.shape[1]-1)],np.array(df_train[\"label\"]))\n",
    "pred = mr.predict(data_test.iloc[:,:(data_train.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == df_test[\"label\"])/len(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopicmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
