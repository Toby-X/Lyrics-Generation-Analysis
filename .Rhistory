for (i in 1:N) {
if (L.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
L.cluster = kmeans(L.evd$vectors[,1:6],K,iter.max = 1e2, nstart=30,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
# D = rowSums(XX)
# L = diag(D^(-1/2))%*%XX%*%diag(D^(-1/2))
# L.off = L-diag(diag(L))
L.off.evd = eigen(XX)
L.off.cluster = kmeans(L.off.evd$vectors[,1:6],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
L.off.cluster = kmeans(L.off.evd$vectors[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
plot(L.off.evd$values)
plot(L.off.evd$values[-1])
L.off.cluster = kmeans(L.off.evd$vectors[,1:4],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
L.off.cluster = kmeans(L.off.evd$vectors[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
L.off.cluster = kmeans(L.off.evd$vectors[,1:2],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
XX.off = XX-diag(diag(XX))
# D = rowSums(XX.off)
# L = diag(D^(-1/2))%*%XX%*%diag(D^(-1/2))
# L.off = L-diag(diag(L))
L.off.evd = svd(XX.off)
L.off.cluster = kmeans(L.off.evd$u[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
plot(L.off.evd$d)
plot(L.off.evd$d[-1])
XX.off = XX-diag(diag(XX))
D = rowSums(XX.off)
L = diag((D+1e-14)^(-1/2))%*%XX.off%*%diag((D+1e-14)^(-1/2))
L.evd = eigen(L)
L.cluster = kmeans(L.evd$vectors[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
knitr::opts_chunk$set(echo = TRUE)
brewer = read.table("brewer_data.txt",header=T,sep = "\t")
reviewer = read.table("reviewer_data.txt",header=T)
library(tidyverse)
brewer.dist = dist(brewer[,4:7],method = "euclidean")
hclust.complete = hclust(brewer.dist,method = "complete")
idx = cutree(hclust.complete,k=20)
plot(hclust.complete)
summary(as.factor(idx))
summary(as.factor(brewer$style))
hclust.single = hclust(brewer.dist,method = "single")
idx = cutree(hclust.single,k=20)
plot(hclust.single)
summary(as.factor(idx))
hclust.average = hclust(brewer.dist,method = "average")
idx = cutree(hclust.average,k=20)
plot(hclust.average)
idx = cutree(hclust.average,k=20)
summary(as.factor(idx))
hclust.median = hclust(brewer.dist,method = "median")
idx = cutree(hclust.median,k=20)
plot(hclust.median)
summary(as.factor(idx))
hclust.centroid = hclust(brewer.dist,method = "centroid")
idx = cutree(hclust.centroid,k=20)
plot(hclust.centroid)
idx = cutree(hclust.centroid,k=20)
summary(as.factor(idx))
hclust.mcquitty = hclust(reviewer.dist,method = "mcquitty")
brewer.dist = dist(brewer[,4:7],method = "euclidean")
reviewer.dist = dist(reviewer[,2:5],method = "euclidean")
hclust.complete = hclust(brewer.dist,method = "complete")
idx = cutree(hclust.complete,k=20)
plot(hclust.complete)
hclust.mcquitty = hclust(reviewer.dist,method = "mcquitty")
idx = cutree(hclust.mcquitty,k=3)
plot(hclust.mcquitty)
hclust.ward.D2 = hclust(reviewer.dist,method = "ward.D2")
idx = cutree(hclust.ward.D2,k=3)
plot(hclust.ward.D2)
hclust.mcquitty = hclust(brewer.dist,method = "mcquitty")
idx = cutree(brewer.mcquitty,k=20)
hclust.mcquitty = hclust(brewer.dist,method = "mcquitty")
idx = cutree(hclust.mcquitty,k=20)
plot(hclust.mcquitty)
summary(as.factor(idx))
?dist
reticulate::repl_python()
install.packages("topicmodels")
library(topicmodels)
install.packages("textmineR")
setwd("D:/Programmes/Python/Lyrics-Generation-Analysis")
#-*-coding:utf-8-*-
dat_train = read.csv("data/train_tfidf_data.csv")
dat_train = dat_train[,-1]
dat_test = read.csv("data/test_tfidf_data.csv")
dat_test = dat_test[,-1]
library(glmnet)
# base model
# perform cross validation on elastic net
a = seq(from=0,to=.02,length=50)
err_cal <- function(pred,k){
sum((as.numeric(pred)-dat_train[idx==k,ncol(dat_train)])^2)/sum(idx==k)
}
err = matrix(rep(0,length(a)*length(lam)),nrow=length(a))
# base model
# perform cross validation on elastic net
a = seq(from=0,to=.1,length=50)
lam = log(seq(from = exp(0.4), to = exp(.8), length=100))
idx = rep(1:5,length.out=nrow(dat_train))
set.seed(516)
idx = sample(idx)
err = matrix(rep(0,length(a)*length(lam)),nrow=length(a))
err.tmp = matrix(rep(0,5*length(lam)),nrow=5)
?outer
?melt
outer(1:3,4:6)
for (i in 1:length(a)){
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = a[i],lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=1)
}
err[i,] = colMeans(err.tmp)
}
persp(a,lam,err)
persp(lam,a,err)
persp(a,lam,err)
?persp
persp(a,lam,err,theta=180)
plot(a,rowMeans(err))
plot(lam,err[1,])
# choose alpha = 0(ridge) and lambda =
lam = log(seq(from = exp(0.7), to = exp(1.2), length=100))
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = 0,lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=1)
}
err[1,] = colMeans(err.tmp)
plot(lam,err[1,])
plot(lam,err[2,])
a[0]
a[1]
# choose alpha = 0(ridge) and lambda =
lam = log(seq(from = exp(1), to = exp(10), length=1000))
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = 0,lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=1)
}
err = matrix(rep(0,length(a)*length(lam)),nrow=length(a))
err.tmp = matrix(rep(0,5*length(lam)),nrow=5)
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = 0,lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=1)
}
err[1,] = colMeans(err.tmp)
plot(lam,err[2,])
plot(lam,err[1,])
# base model
# perform cross validation on elastic net
a = seq(from=0,to=.1,length=100)
lam = log(seq(from = exp(0.4), to = exp(.8), length=100))
i = 1
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = a[i],lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
str(pre.tmp)
str(as.numeric(pre.tmp))
str(pre.tmp[,1])
str(pre.tmp[,0])
str(dat_train[idx==k,ncol(dat_train)])
dat_train[idx==k,ncol(dat_train)]
k
idx==k
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=1)
err.tmp[k,]
err.tmp[k,1]
# base model
# perform cross validation on elastic net
a = seq(from=0,to=.1,length=100)
lam = log(seq(from = exp(0.4), to = exp(.8), length=100))
i = 1
k = 1
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = a[i],lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=1)
err.tmp[k,1]
pred = pre.tmp[,1]
sum((as.numeric(pred)-dat_train[idx==k,ncol(dat_train)])^2)/sum(idx==k)
pred = pre.tmp[,2]
sum((as.numeric(pred)-dat_train[idx==k,ncol(dat_train)])^2)/sum(idx==k)
err.tmp[k,2]
err = matrix(rep(0,length(a)*length(lam)),nrow=length(a))
err.tmp = matrix(rep(0,5*length(lam)),nrow=5)
for (i in 1:length(a)){
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = a[i],lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=i)
}
err[i,] = colMeans(err.tmp)
}
for (i in 1:length(a)){
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = a[i],lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=k)
}
err[i,] = colMeans(err.tmp)
}
plot(a,rowMeans(err))
persp(a,lamb,err)
persp(a,lam,err)
plot(a,rowMeans(err),theta=180)
persp(a,lam,err,theta=180)
warnings()
plot(lam,err[1,])
plot(lam,err[2,])
plot(lam,err[3,])
plot(lam,err[4,])
plot(lam,err[5,])
# choose alpha = 0(ridge) and lambda =
lam = log(seq(from = exp(.7), to = exp(1.5), length=1000))
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = 0,lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=1)
}
# choose alpha = 0(ridge) and lambda =
lam = log(seq(from = exp(.7), to = exp(1.5), length=100))
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = 0,lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=1)
}
err[1,] = colMeans(err.tmp)
plot(lam,err[1,])
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = 0,lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=i)
}
err[1,] = colMeans(err.tmp)
plot(lam,err[1,])
plot(lam,err[1,])
err[1,]
err.tmp
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = 0,lambda = lam)
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=k)
}
err[1,] = colMeans(err.tmp)
plot(lam,err[1,])
plot(lam,err[2,])
plot(lam,err[1,])
str(pre.tmp)
pre.tmp[,1]
pre.tmp[,2]
pre.tmp[,50]
pre.tmp[,100]
?cv.glmnet
?glmnet
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = a[i],lambda = lam[i])
pred = pre.tmp
mean((as.numeric(pred)-dat_train[idx==k,ncol(dat_train)])^2)
glmnet_cv <- function(i,j){
for (k in 1:5){
mr.tmp = glmnet(data.matrix(dat_train[idx!=k,-ncol(dat_train)]),as.factor(dat_train[idx!=k,ncol(dat_train)])
,family="multinomial",alpha = a[i],lambda = lam[j])
pre.tmp = predict(mr.tmp,data.matrix(dat_train[idx==k,-ncol(dat_train)]),type="class")
err.tmp[k] = mean((as.numeric(pred)-dat_train[idx==k,ncol(dat_train)])^2)
}
err[i,j] = mean(err.tmp)
}
err = matrix(rep(0,length(a)*length(lam)),nrow=length(a))
err.tmp = rep(0,5)
# base model
# perform cross validation on elastic net
a = seq(from=0,to=.1,length=100)
lam = log(seq(from = exp(0.4), to = exp(.8), length=100))
err = matrix(rep(0,length(a)*length(lam)),nrow=length(a))
err.tmp = rep(0,5)
outer(1:length(a),1:length(lam),glmnet_cv)
lam[j]
lam[1]
?outer
# SVM
library(kernlab)
?svm
?ksvm
ksvm(data.matrix(dat_train[,-ncol(dat_train)]),as.factor(dat_train[,ncol(dat_train)]),
kernel=rbfdot())
svm = ksvm(data.matrix(dat_train[,-ncol(dat_train)]),as.factor(dat_train[,ncol(dat_train)]))
predict(svm,data.matrix(dat_test[,-ncol(dat_test)]))
pre = predict(svm,data.matrix(dat_test[,-ncol(dat_test)]))
sum(pre==dat_test[,ncol(dat_test)])/length(pre)
summary(svm)
svm
