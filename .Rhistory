}
}
err = err/N
err
# D = rowSums(XX)
# L = diag(D^(-1/2))%*%XX%*%diag(D^(-1/2))
# L.off = L-diag(diag(L))
L.off.evd = eigen(XX)
L.off.cluster = kmeans(L.off.evd$vectors[,1:6],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
L.off.cluster = kmeans(L.off.evd$vectors[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
plot(L.off.evd$values)
plot(L.off.evd$values[-1])
L.off.cluster = kmeans(L.off.evd$vectors[,1:4],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
L.off.cluster = kmeans(L.off.evd$vectors[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
L.off.cluster = kmeans(L.off.evd$vectors[,1:2],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
XX.off = XX-diag(diag(XX))
# D = rowSums(XX.off)
# L = diag(D^(-1/2))%*%XX%*%diag(D^(-1/2))
# L.off = L-diag(diag(L))
L.off.evd = svd(XX.off)
L.off.cluster = kmeans(L.off.evd$u[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
plot(L.off.evd$d)
plot(L.off.evd$d[-1])
XX.off = XX-diag(diag(XX))
D = rowSums(XX.off)
L = diag((D+1e-14)^(-1/2))%*%XX.off%*%diag((D+1e-14)^(-1/2))
L.evd = eigen(L)
L.cluster = kmeans(L.evd$vectors[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
knitr::opts_chunk$set(echo = TRUE)
brewer = read.table("brewer_data.txt",header=T,sep = "\t")
reviewer = read.table("reviewer_data.txt",header=T)
library(tidyverse)
brewer.dist = dist(brewer[,4:7],method = "euclidean")
hclust.complete = hclust(brewer.dist,method = "complete")
idx = cutree(hclust.complete,k=20)
plot(hclust.complete)
summary(as.factor(idx))
summary(as.factor(brewer$style))
hclust.single = hclust(brewer.dist,method = "single")
idx = cutree(hclust.single,k=20)
plot(hclust.single)
summary(as.factor(idx))
hclust.average = hclust(brewer.dist,method = "average")
idx = cutree(hclust.average,k=20)
plot(hclust.average)
idx = cutree(hclust.average,k=20)
summary(as.factor(idx))
hclust.median = hclust(brewer.dist,method = "median")
idx = cutree(hclust.median,k=20)
plot(hclust.median)
summary(as.factor(idx))
hclust.centroid = hclust(brewer.dist,method = "centroid")
idx = cutree(hclust.centroid,k=20)
plot(hclust.centroid)
idx = cutree(hclust.centroid,k=20)
summary(as.factor(idx))
hclust.mcquitty = hclust(reviewer.dist,method = "mcquitty")
brewer.dist = dist(brewer[,4:7],method = "euclidean")
reviewer.dist = dist(reviewer[,2:5],method = "euclidean")
hclust.complete = hclust(brewer.dist,method = "complete")
idx = cutree(hclust.complete,k=20)
plot(hclust.complete)
hclust.mcquitty = hclust(reviewer.dist,method = "mcquitty")
idx = cutree(hclust.mcquitty,k=3)
plot(hclust.mcquitty)
hclust.ward.D2 = hclust(reviewer.dist,method = "ward.D2")
idx = cutree(hclust.ward.D2,k=3)
plot(hclust.ward.D2)
hclust.mcquitty = hclust(brewer.dist,method = "mcquitty")
idx = cutree(brewer.mcquitty,k=20)
hclust.mcquitty = hclust(brewer.dist,method = "mcquitty")
idx = cutree(hclust.mcquitty,k=20)
plot(hclust.mcquitty)
summary(as.factor(idx))
?dist
reticulate::repl_python()
install.packages("topicmodels")
library(topicmodels)
install.packages("textmineR")
library(torch)
library(topicmodels.etm)
library(doc2vec)
library(word2vec)
data("be_parliament_2020",package = "doc2vec")
x <- data.frame(doc_id=be_parliament_2020$doc_id,
text = be_parliament_2020$text_nl,
stringsAsFactors = F)
x$text <- txt_clean_word2vec(x$text)
head(x$text)
w2v = word2vec(x=x$text,dim=25,type = "skip-gram",
iter=10, min_count = 5, threads = 2)
?word2vec
embeddings = as.matrix(w2v)
predict(w2v,newdata=c("migranten","belastingen"),
type="nearest",top_n=4)
library(udpipe)
dtm = strsplit.data.frame(x,group="doc_id",term="text",split=" ")
dtm = document_term_frequencies(dtm)
dtm = document_term_matrix(dtm,prob=.5)
?document_term_matrix
vocab = intersect(rownames(embeddings),colnames(dtm))
embeddings = dtm_conform(embeddings,rows=vocab)
dtm = dtm_conform(dtm,columns = vocab)
set.seed(1234)
torch_manual_seed(4321)
model = ETM(k=20, dim=100, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
newdata <- head(dtm, n = 5)
scores  <- predict(model, newdata, type = "topics")
scores
library(textplot)
library(uwot)
library(ggrepel)
library(ggalt)
manifolded <- summary(model, type = "umap", n_components = 2, metric = "cosine", n_neighbors = 15,
fast_sgd = FALSE, n_threads = 2, verbose = TRUE)
space      <- subset(manifolded$embed_2d, type %in% "centers")
textplot_embedding_2d(space)
manifolded <- summary(model, type = "umap", n_components = 2, metric = "cosine", n_neighbors = 15,
fast_sgd = FALSE, n_threads = 2, verbose = TRUE)
space      <- subset(manifolded$embed_2d, type %in% "centers")
textplot_embedding_2d(space)
space      <- subset(manifolded$embed_2d, cluster %in% c(12, 14, 9, 7) & rank <= 7)
textplot_embedding_2d(space, title = "ETM topics", subtitle = "embedded in 2D using UMAP",
encircle = FALSE, points = TRUE)
#-*- coding:utf-8 -*-
library(torch)
library(topicmodels.etm)
library(doc2vec)
library(word2vec)
library(data.table)
setwd("D:/Programmes/Python/Lyrics-Generation-Analysis")
dat = fread("data/billboard_all_lyrics.csv")
x = data.frame(song_id = dat$song_id,
lyrics = dat$lyrics,
stringsAsFactors = F)
x = data.frame(song_id = dat$song_id,
lyrics = dat$lyrics,
stringsAsFactors = F)
x$lyrics = txt_clean_word2vec(x$lyrics)
head(x$lyrics)
w2v = word2vec(x=x$lyrics,dim=25,type = "skip-gram",
iter=10, min_count = 5, threads = 4)
embeddings = as.matrix(w2v)
predict(w2v,newdata=c("way","weave"),
type="nearest",top_n=4)
library(udpipe)
dtm = strsplit.data.frame(x,group="song_id",term="lyrics",split=" ")
dtm = document_term_frequencies(dtm)
dtm = document_term_matrix(dtm,prob=.5)
vocab = intersect(rownames(embeddings),colnames(dtm))
embeddings = dtm_conform(embeddings,rows=vocab)
dtm = dtm_conform(dtm,columns = vocab)
set.seed(1234)
torch_manual_seed(4321)
?ETM
model = ETM(k=6, dim=100, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
terminology = predict(model,type="terms",top_n = 5)
terminology
?predict.ETM
terminology = predict(model,type="terms",top_n = 20)
terminology
library(textplot)
library(uwot)
library(ggrepel)
library(ggalt)
manifolded <- summary(model, type = "umap", n_components = 2, metric = "cosine", n_neighbors = 15,
fast_sgd = FALSE, n_threads = 2, verbose = TRUE)
space      <- subset(manifolded$embed_2d, type %in% "centers")
textplot_embedding_2d(space)
space      <- subset(manifolded$embed_2d, cluster %in% c(12, 14, 9, 7) & rank <= 7)
textplot_embedding_2d(space, title = "ETM topics", subtitle = "embedded in 2D using UMAP",
encircle = FALSE, points = TRUE)
space      <- subset(manifolded$embed_2d, type %in% "centers")
textplot_embedding_2d(space)
space      <- subset(manifolded$embed_2d, rank <= 7)
textplot_embedding_2d(space, title = "ETM topics", subtitle = "embedded in 2D using UMAP",
encircle = T, points = TRUE)
model = ETM(k=20, dim=100, embeddings = embeddings)
model = ETM(k=20, dim=100, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
terminology = predict(model,type="terms",top_n = 20)
terminology
manifolded <- summary(model, type = "umap", n_components = 2, metric = "cosine", n_neighbors = 15,
fast_sgd = FALSE, n_threads = 2, verbose = TRUE)
space      <- subset(manifolded$embed_2d, type %in% "centers")
textplot_embedding_2d(space)
space      <- subset(manifolded$embed_2d, rank <= 15)
textplot_embedding_2d(space, title = "ETM topics", subtitle = "embedded in 2D using UMAP",
encircle = T, points = TRUE)
space      <- subset(manifolded$embed_2d, type %in% "centers")
textplot_embedding_2d(space)
space      <- subset(manifolded$embed_2d, rank <= 15)
textplot_embedding_2d(space, title = "ETM topics", subtitle = "embedded in 2D using UMAP",
encircle = F, points = TRUE)
terminology
?clean
??clean
?txt_clean_word2vec
dat = fread("data/df_cluster.csv")
dat = fread("data/df_cluster.csv")
x = data.frame(song_id = dat$song_id,
lyrics = dat$lyrics,
stringsAsFactors = F)
x$lyrics = txt_clean_word2vec(x$lyrics)
w2v = word2vec(x=x$lyrics,dim=25,type = "skip-gram",
iter=10, min_count = 5, threads = 4)
embeddings = as.matrix(w2v)
predict(w2v,newdata=c("way","weave"),
type="nearest",top_n=4)
dtm = strsplit.data.frame(x,group="song_id",term="lyrics",split=" ")
dtm = document_term_frequencies(dtm)
dtm = document_term_matrix(dtm,prob=.5)
vocab = intersect(rownames(embeddings),colnames(dtm))
embeddings = dtm_conform(embeddings,rows=vocab)
dtm = dtm_conform(dtm,columns = vocab)
set.seed(1234)
torch_manual_seed(4321)
model = ETM(k=6, dim=100, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
terminology = predict(model,type="terms",top_n = 20)
terminology
manifolded <- summary(model, type = "umap", n_components = 2, metric = "cosine", n_neighbors = 15,
fast_sgd = FALSE, n_threads = 2, verbose = TRUE)
space      <- subset(manifolded$embed_2d, type %in% "centers")
textplot_embedding_2d(space)
space      <- subset(manifolded$embed_2d, rank <= 15)
textplot_embedding_2d(space, title = "ETM topics", subtitle = "embedded in 2D using UMAP",
encircle = F, points = TRUE)
w2v = word2vec(x=x$lyrics,dim=25,type = "cbow",
iter=10, min_count = 5, threads = 4)
embeddings = as.matrix(w2v)
predict(w2v,newdata=c("way","weave"),
type="nearest",top_n=4)
library(udpipe)
dtm = strsplit.data.frame(x,group="song_id",term="lyrics",split=" ")
dtm = document_term_frequencies(dtm)
dtm = document_term_matrix(dtm,prob=.5)
vocab = intersect(rownames(embeddings),colnames(dtm))
embeddings = dtm_conform(embeddings,rows=vocab)
dtm = dtm_conform(dtm,columns = vocab)
set.seed(1234)
torch_manual_seed(4321)
model = ETM(k=6, dim=100, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
terminology = predict(model,type="terms",top_n = 20)
terminology
w2v = word2vec(x=x$lyrics,dim=50,type = "cbow",
iter=30, min_count = 20, threads = 6)
embeddings = as.matrix(w2v)
predict(w2v,newdata=c("way","weave"),
type="nearest",top_n=4)
dtm = strsplit.data.frame(x,group="song_id",term="lyrics",split=" ")
dtm = document_term_frequencies(dtm)
dtm = document_term_matrix(dtm,prob=.5)
vocab = intersect(rownames(embeddings),colnames(dtm))
embeddings = dtm_conform(embeddings,rows=vocab)
dtm = dtm_conform(dtm,columns = vocab)
set.seed(1234)
torch_manual_seed(4321)
?ETM
model = ETM(k=6, dim=800, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
terminology = predict(model,type="terms",top_n = 20)
terminology
dat = fread("data/df_cluster.csv")
x = data.frame(song_id = dat$song_id,
lyrics = dat$lyrics,
stringsAsFactors = F)
x$lyrics = txt_clean_word2vec(x$lyrics)
w2v = word2vec(x=x$lyrics,dim=50,type = "cbow",
iter=30, min_count = 20, threads = 6)
embeddings = as.matrix(w2v)
predict(w2v,newdata=c("way","weave"),
type="nearest",top_n=4)
library(udpipe)
dtm = strsplit.data.frame(x,group="song_id",term="lyrics",split=" ")
dtm = document_term_frequencies(dtm)
dtm = document_term_matrix(dtm,prob=.5)
vocab = intersect(rownames(embeddings),colnames(dtm))
embeddings = dtm_conform(embeddings,rows=vocab)
dtm = dtm_conform(dtm,columns = vocab)
set.seed(1234)
torch_manual_seed(4321)
model = ETM(k=6, dim=800, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
terminology = predict(model,type="terms",top_n = 20)
terminology
model = ETM(k=20, dim=800, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
terminology = predict(model,type="terms",top_n = 20)
terminology
?word2vec
w2v = word2vec(x=x$lyrics,dim=50,type = "skip-gram",
iter=30, min_count = 20, threads = 6)
embeddings = as.matrix(w2v)
predict(w2v,newdata=c("way","weave"),
type="nearest",top_n=4)
library(udpipe)
dtm = strsplit.data.frame(x,group="song_id",term="lyrics",split=" ")
dtm = document_term_frequencies(dtm)
dtm = document_term_matrix(dtm,prob=.5)
vocab = intersect(rownames(embeddings),colnames(dtm))
embeddings = dtm_conform(embeddings,rows=vocab)
dtm = dtm_conform(dtm,columns = vocab)
set.seed(1234)
torch_manual_seed(4321)
model = ETM(k=3, dim=800, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
terminology = predict(model,type="terms",top_n = 20)
terminology
set.seed(1234)
torch_manual_seed(4321)
model = ETM(k=6, dim=800, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
terminology = predict(model,type="terms",top_n = 20)
terminology
model = ETM(k=250, dim=800, embeddings = embeddings)
optimizer = optim_adam(params=model$parameters,lr=.005,weight_decay = .0000012)
loss = model$fit(data=dtm,optimizer = optimizer,epoch=20,batch_size = 1e3)
plot(model, type = "loss")
terminology = predict(model,type="terms",top_n = 20)
terminology
