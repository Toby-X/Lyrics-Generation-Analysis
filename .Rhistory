for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
L.off.cluster = kmeans(L.off.evd$vectors[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
plot(L.off.evd$values)
plot(L.off.evd$values[-1])
L.off.cluster = kmeans(L.off.evd$vectors[,1:4],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
L.off.cluster = kmeans(L.off.evd$vectors[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
L.off.cluster = kmeans(L.off.evd$vectors[,1:2],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
XX.off = XX-diag(diag(XX))
# D = rowSums(XX.off)
# L = diag(D^(-1/2))%*%XX%*%diag(D^(-1/2))
# L.off = L-diag(diag(L))
L.off.evd = svd(XX.off)
L.off.cluster = kmeans(L.off.evd$u[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.off.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.off.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.off.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.off.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.off.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
plot(L.off.evd$d)
plot(L.off.evd$d[-1])
XX.off = XX-diag(diag(XX))
D = rowSums(XX.off)
L = diag((D+1e-14)^(-1/2))%*%XX.off%*%diag((D+1e-14)^(-1/2))
L.evd = eigen(L)
L.cluster = kmeans(L.evd$vectors[,1:5],K,iter.max = 1e2, nstart=100,algorithm = "Lloyd")
Pi.est = matrix(rep(0,N*K),nrow=N)
for (i in 1:N) {
if (L.cluster$cluster[i] == 1){
Pi.est[i,1] = 1
} else if (L.cluster$cluster[i] == 2) {
Pi.est[i,2] = 1
} else if (L.cluster$cluster[i] == 3) {
Pi.est[i,3] = 1
} else if (L.cluster$cluster[i] == 4) {
Pi.est[i,4] = 1
} else if (L.cluster$cluster[i] == 5) {
Pi.est[i,5] = 1
}
}
idx.est = order(colMeans(Pi.est))
err = 0
for (i in 1:N) {
if (any(Pi.est[i,idx.est]!=Pi.r[i,idx.ori])){
err = err+1
}
}
err = err/N
err
knitr::opts_chunk$set(echo = TRUE)
brewer = read.table("brewer_data.txt",header=T,sep = "\t")
reviewer = read.table("reviewer_data.txt",header=T)
library(tidyverse)
brewer.dist = dist(brewer[,4:7],method = "euclidean")
hclust.complete = hclust(brewer.dist,method = "complete")
idx = cutree(hclust.complete,k=20)
plot(hclust.complete)
summary(as.factor(idx))
summary(as.factor(brewer$style))
hclust.single = hclust(brewer.dist,method = "single")
idx = cutree(hclust.single,k=20)
plot(hclust.single)
summary(as.factor(idx))
hclust.average = hclust(brewer.dist,method = "average")
idx = cutree(hclust.average,k=20)
plot(hclust.average)
idx = cutree(hclust.average,k=20)
summary(as.factor(idx))
hclust.median = hclust(brewer.dist,method = "median")
idx = cutree(hclust.median,k=20)
plot(hclust.median)
summary(as.factor(idx))
hclust.centroid = hclust(brewer.dist,method = "centroid")
idx = cutree(hclust.centroid,k=20)
plot(hclust.centroid)
idx = cutree(hclust.centroid,k=20)
summary(as.factor(idx))
hclust.mcquitty = hclust(reviewer.dist,method = "mcquitty")
brewer.dist = dist(brewer[,4:7],method = "euclidean")
reviewer.dist = dist(reviewer[,2:5],method = "euclidean")
hclust.complete = hclust(brewer.dist,method = "complete")
idx = cutree(hclust.complete,k=20)
plot(hclust.complete)
hclust.mcquitty = hclust(reviewer.dist,method = "mcquitty")
idx = cutree(hclust.mcquitty,k=3)
plot(hclust.mcquitty)
hclust.ward.D2 = hclust(reviewer.dist,method = "ward.D2")
idx = cutree(hclust.ward.D2,k=3)
plot(hclust.ward.D2)
hclust.mcquitty = hclust(brewer.dist,method = "mcquitty")
idx = cutree(brewer.mcquitty,k=20)
hclust.mcquitty = hclust(brewer.dist,method = "mcquitty")
idx = cutree(hclust.mcquitty,k=20)
plot(hclust.mcquitty)
summary(as.factor(idx))
?dist
reticulate::repl_python()
install.packages("topicmodels")
library(topicmodels)
install.packages("textmineR")
# library(data.table)
# library(tictoc)
# library(parallel)
# library(foreach)
# library(doSNOW)
# numCores = 6
# cl = makeCluster(numCores)
# registerDoSNOW(cl)
dat_train = read.csv("data/train_data_all.csv")
setwd("D:/Programmes/Python/Lyrics-Generation-Analysis")
#-*-coding:utf-8-*-
library(glmnet)
library(kernlab)
# library(data.table)
# library(tictoc)
# library(parallel)
# library(foreach)
# library(doSNOW)
# numCores = 6
# cl = makeCluster(numCores)
# registerDoSNOW(cl)
dat_train = read.csv("data/train_data_all.csv")
dat_train = dat_train[,-1]
dat_test = read.csv("data/test_data_all.csv")
dat_test = dat_test[,-1]
train_other = read.csv("data/train_other.csv")
colnames(dat_train)[ncol(dat_train)-2]
colnames(dat_train)[ncol(dat_train)-1]
# normalization of word frequency
freq_mean = colMeans(dat_train[,14:(ncol(dat_train)-1)])
freq_sd = apply(dat_train[,14:(ncol(dat_train)-1)],2,sd)
dat_train[,14:(ncol(dat_train)-1)] = t((t(dat_train[,14:(ncol(dat_train)-1)])-freq_mean)/freq_sd)
dat_test[,14:(ncol(dat_test)-1)] = t((t(dat_test[,14:(ncol(dat_test)-1)])-freq_mean)/freq_sd)
# base model
# perform cross validation on elastic net
lam = log(seq(from = exp(1e-2), to = exp(1.5), length=100))
idx = rep(1:5,length.out=nrow(dat_train))
set.seed(516)
idx = sample(idx)
# normalization of word frequency
freq_mean = colMeans(dat_train[,14:(ncol(dat_train)-1)])
freq_sd = apply(dat_train[,14:(ncol(dat_train)-1)],2,sd)
dat_train[,14:(ncol(dat_train)-1)] = t((t(dat_train[,14:(ncol(dat_train)-1)])-freq_mean)/freq_sd)
dat_test[,14:(ncol(dat_test)-1)] = t((t(dat_test[,14:(ncol(dat_test)-1)])-freq_mean)/freq_sd)
# base model
# perform cross validation on elastic net
lam = log(seq(from = exp(1e-2), to = exp(1.5), length=100))
idx = rep(1:5,length.out=nrow(dat_train))
set.seed(516)
idx = sample(idx)
ch2int <- function(ch){
if (ch == 'FALSE'){
return(0)
} else {
return(1)
}
}
err_cal <- function(pred,k){
pred = sapply(pred, ch2int)
mean((pred!=(dat_train[idx==k,ncol(dat_train)])))
}
err.tmp = matrix(rep(0,5*length(lam)),nrow=5)
for (k in 1:5) {
set.seed(k)
corr = apply(dat_train[idx!=k,14:(ncol(dat_train)-3)],2,cor,y=train_other$active_years)
idx_avail = abs(corr)>.05
dat_train_word = dat_train[idx!=k,14:(ncol(dat_train)-3)]
dat_train_word = dat_train_word[idx!=k,idx_avail]
dat_train_fil = cbind(dat_train[idx!=k,1:13],dat_train_word,dat_train[idx!=k,(ncol(dat_train)-2):ncol(dat_train)])
clf.tmp = glmnet(data.matrix(dat_train_fil[idx!=k,-ncol(dat_train_fil)]),as.factor(dat_train_fil[idx!=k,ncol(dat_train_fil)]),
alpha = 0,lambda = lam,family = "multinomial")
dat_test_word = dat_test[idx==k,14:(ncol(dat_test)-3)]
dat_test_word = dat_test_word[idx==k,idx_avail]
dat_test_fil = cbind(dat_test[idx==k,1:13],dat_test_word,dat_test[idx==k,(ncol(dat_test)-2):ncol(dat_test)])
pred.tmp = predict(clf.tmp,data.matrix(dat_test_fil[,-ncol(dat_test_fil)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=k)
}
k
corr = apply(dat_train[idx!=k,14:(ncol(dat_train)-3)],2,cor,y=train_other$active_years)
corr = apply(dat_train[idx!=k,14:(ncol(dat_train)-3)],2,cor,y=train_other$active_years[idx!=k])
warnings()
corr = apply(dat_train[idx!=k,14:(ncol(dat_train)-3)],2,cor,y=train_other$active_years[idx!=k])
corr
boxplot(corr)
sum(abs(corr)>.05)
is.na(corr)
any(is.na(corr))
corr[is.na(corr)]
corr = apply(dat_train[idx!=k,14:(ncol(dat_train)-3)],2,cor,y=train_other$active_years[idx!=k])
if (any(is.na(corr))){
corr[is.na(corr)] = 0
}
idx_avail = abs(corr)>.05
dat_train_word = dat_train[idx!=k,14:(ncol(dat_train)-3)]
sum(idx_avail>0)
dat_train_word = dat_train_word[idx!=k,idx_avail]
dat_train_fil = cbind(dat_train[idx!=k,1:13],dat_train_word,dat_train[idx!=k,(ncol(dat_train)-2):ncol(dat_train)])
clf.tmp = glmnet(data.matrix(dat_train_fil[idx!=k,-ncol(dat_train_fil)]),as.factor(dat_train_fil[idx!=k,ncol(dat_train_fil)]),
alpha = 0,lambda = lam,family = "multinomial")
dat_train_word
any(is.na(dat_train_word))
dat_train_word[is.na(dat_train_word)]
max(idx_avail)
idx_avail
colnames(dat_train_word)[idx_avail]
1:length(idx_avail)[idx_avail]
(1:length(idx_avail))[idx_avail]
idx1 = (1:length(idx_avail))[idx_avail]
colnames(dat_train_word)[idx1]
colnames(dat_train_word)
idx_avail = abs(corr)>.05
dat_train_word = dat_train[idx!=k,idx_avail]
dat_train_fil = cbind(dat_train[idx!=k,1:13],dat_train_word,dat_train[idx!=k,(ncol(dat_train)-2):ncol(dat_train)])
clf.tmp = glmnet(data.matrix(dat_train_fil[idx!=k,-ncol(dat_train_fil)]),as.factor(dat_train_fil[idx!=k,ncol(dat_train_fil)]),
alpha = 0,lambda = lam,family = "multinomial")
dat_train_fil
any(is.na(dat_train[idx!=k,1:13]))
any(is.na(dat_train_word))
any(is.na(dat_train[idx!=k,(ncol(dat_train)-2):ncol(dat_train)]))
dat_train_fil = cbind(dat_train[idx!=k,1:13],dat_train_word,dat_train[idx!=k,(ncol(dat_train)-2):ncol(dat_train)])
any(is.na(dat_train_fil))
clf.tmp = glmnet(data.matrix(dat_train_fil[idx!=k,-ncol(dat_train_fil)]),as.factor(dat_train_fil[idx!=k,ncol(dat_train_fil)]),
alpha = 0,lambda = lam,family = "multinomial")
View(dat_train_fil)
# normalization of word frequency
freq_mean = max(dat_train[,14:(ncol(dat_train)-1)],axis=1)
dat_train = read.csv("data/train_data_all.csv")
dat_train = dat_train[,-1]
dat_test = read.csv("data/test_data_all.csv")
dat_test = dat_test[,-1]
train_other = read.csv("data/train_other.csv")
# normalization of word frequency
var_max = max(dat_train[,14:(ncol(dat_train)-1)],axis=1)
freq_sd = apply(dat_train[,14:(ncol(dat_train)-1)],2,sd)
dat_train[,14:(ncol(dat_train)-1)] = t(t(dat_train[,14:(ncol(dat_train)-1)])/var_max)
dat_test[,14:(ncol(dat_test)-1)] = t(t(dat_test[,14:(ncol(dat_test)-1)])/var_max)
# base model
# perform cross validation on elastic net
lam = log(seq(from = exp(1e-2), to = exp(1.5), length=100))
idx = rep(1:5,length.out=nrow(dat_train))
set.seed(516)
idx = sample(idx)
ch2int <- function(ch){
if (ch == 'FALSE'){
return(0)
} else {
return(1)
}
}
err_cal <- function(pred,k){
pred = sapply(pred, ch2int)
mean((pred!=(dat_train[idx==k,ncol(dat_train)])))
}
set.seed(k)
corr = apply(dat_train[idx!=k,14:(ncol(dat_train)-3)],2,cor,y=train_other$active_years[idx!=k])
warnings
warnings()
if (any(is.na(corr))){
corr[is.na(corr)] = 0
}
idx_avail = abs(corr)>.05
dat_train_word = dat_train[idx!=k,idx_avail]
dat_train_fil = cbind(dat_train[idx!=k,1:13],dat_train_word,dat_train[idx!=k,(ncol(dat_train)-2):ncol(dat_train)])
clf.tmp = glmnet(data.matrix(dat_train_fil[idx!=k,-ncol(dat_train_fil)]),as.factor(dat_train_fil[idx!=k,ncol(dat_train_fil)]),
alpha = 0,lambda = lam,family = "multinomial")
any(is.na(dat_train_fil))
View(dat_train_fil)
?makeX
clf.tmp = glmnet(data.matrix(dat_train_fil[,-ncol(dat_train_fil)]),as.factor(dat_train_fil[,ncol(dat_train_fil)]),
alpha = 0,lambda = lam,family = "multinomial")
dat_test_word = dat_test[idx==k,14:(ncol(dat_test)-3)]
dat_test_word = dat_test_word[idx==k,idx_avail]
dat_test_fil = cbind(dat_test[idx==k,1:13],dat_test_word,dat_test[idx==k,(ncol(dat_test)-2):ncol(dat_test)])
pred.tmp = predict(clf.tmp,data.matrix(dat_test_fil[,-ncol(dat_test_fil)]),type="class")
dat_test_word = dat_test[idx==k,idx_avail]
dat_test_fil = cbind(dat_test[idx==k,1:13],dat_test_word,dat_test[idx==k,(ncol(dat_test)-2):ncol(dat_test)])
pred.tmp = predict(clf.tmp,data.matrix(dat_test_fil[,-ncol(dat_test_fil)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=k)
pre.tmp = predict(clf.tmp,data.matrix(dat_test_fil[,-ncol(dat_test_fil)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=k)
head(pre.tmp)
"0"==0
err_cal <- function(pred,k){
mean((pred!=(dat_train[idx==k,ncol(dat_train)])))
}
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=k)
plot(err.tmp[k,])
err.tmp[k,]
head(pre.tmp)
head(pre.tmp[,1])
head(dat_train[idx==k,ncol(dat_train)])
mean(pre.tmp[,1]!=dat_train[idx==k,ncol(dat_train)])
sum(pre.tmp[,1]!=dat_train[idx==k,ncol(dat_train)])
pre.tmp[,1]!=dat_train[idx==k,ncol(dat_train)]
str(pre.tmp[,1])
str(dat_train[idx==k,ncol(dat_train)])
5=="5"
5!="5"
4!="5"
pre.tmp[902,1]
dat_test_word = dat_train[idx==k,idx_avail]
dat_test_word = dat_train[idx==k,idx_avail]
dat_test_fil = cbind(dat_train[idx==k,1:13],dat_test_word,dat_train[idx==k,(ncol(dat_train)-2):ncol(dat_train)])
pre.tmp = predict(clf.tmp,data.matrix(dat_test_fil[,-ncol(dat_test_fil)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=k)
err.tmp[k,]
corr = apply(dat_train[,14:(ncol(dat_train)-2)],2,cor,y=train_other$active_years)
boxplot(corr)
sum(abs(corr)>0.05)
idx_avail = abs(corr)>.05
dat_train_word = dat_train_word[,idx_avail]
dat_train_word = dat_train[,idx_avail]
dat_train_fil = cbind(dat_train[,1:13],dat_train_word,dat_train[,(ncol(dat_train)-1):ncol(dat_train)])
clf.tmp = glmnet(data.matrix(dat_train_fil[,-ncol(dat_train_fil)]),as.factor(dat_train_fil[,ncol(dat_train_fil)]),
alpha = 0,lambda = .5,family = "multinomial")
dat_test_word = dat_test[,idx_avail]
dat_test_fil = cbind(dat_test[,1:13],dat_test_word,dat_test[,(ncol(dat_test)-1):ncol(dat_test)])
pred = predict(clf.tmp,data.matrix(dat_test_fil[,-ncol(dat_test_fil)]),type="class")
mean(pred == dat_test_fil[,ncol(dat_test_fil)])
sum(abs(corr)>0.025)
idx_avail = abs(corr)>.05
idx_avail = abs(corr)>.025
dat_train_word = dat_train[,idx_avail]
dat_train_fil = cbind(dat_train[,1:13],dat_train_word,dat_train[,(ncol(dat_train)-1):ncol(dat_train)])
clf.tmp = glmnet(data.matrix(dat_train_fil[,-ncol(dat_train_fil)]),as.factor(dat_train_fil[,ncol(dat_train_fil)]),
alpha = 0,lambda = .5,family = "multinomial")
dat_test_word = dat_test[,idx_avail]
dat_test_fil = cbind(dat_test[,1:13],dat_test_word,dat_test[,(ncol(dat_test)-1):ncol(dat_test)])
pred = predict(clf.tmp,data.matrix(dat_test_fil[,-ncol(dat_test_fil)]),type="class")
mean(pred == dat_test_fil[,ncol(dat_test_fil)])
sum(pred==0)
sum(dat_test_fil[,ncol(dat_test_fil)]==5)
svm.tmp = ksvm(data.matrix(dat_train_fil[,-ncol(dat_train_fil)]),as.factor(dat_train_fil[,ncol(dat_train_fil)]),
kernel="rbfdot",kpar="automatic")
pred = predict(svm.tmp,dat_test_fil[,-ncol(dat_test_fil)],type="response")
mean(pred == dat_test_fil[,ncol(dat_test_fil)])
var_max = max(dat_train[,14:(ncol(dat_train)-1)],axis=1)
dat_train[,14:(ncol(dat_train)-1)] = t(t(dat_train[,14:(ncol(dat_train)-1)])/var_max)
dat_test[,14:(ncol(dat_test)-1)] = t(t(dat_test[,14:(ncol(dat_test)-1)])/var_max)
# base model
# perform cross validation on elastic net
lam = log(seq(from = exp(1e-2), to = exp(5), length=500))
idx = rep(1:5,length.out=nrow(dat_train))
set.seed(516)
idx = sample(idx)
ch2int <- function(ch){
if (ch == 'FALSE'){
return(0)
} else {
return(1)
}
}
err_cal <- function(pred,k){
mean((pred!=(dat_train[idx==k,ncol(dat_train)])))
}
err.tmp = matrix(rep(0,5*length(lam)),nrow=5)
for (k in 1:5) {
set.seed(k)
corr = apply(dat_train[idx!=k,14:(ncol(dat_train)-3)],2,cor,y=train_other$active_years[idx!=k])
if (any(is.na(corr))){
corr[is.na(corr)] = 0
}
idx_avail = abs(corr)>.05
dat_train_word = dat_train[idx!=k,idx_avail]
dat_train_fil = cbind(dat_train[idx!=k,1:13],dat_train_word,dat_train[idx!=k,(ncol(dat_train)-2):ncol(dat_train)])
clf.tmp = glmnet(data.matrix(dat_train_fil[,-ncol(dat_train_fil)]),as.factor(dat_train_fil[,ncol(dat_train_fil)]),
alpha = 0,lambda = lam,family = "multinomial")
dat_test_word = dat_train[idx==k,idx_avail]
dat_test_fil = cbind(dat_train[idx==k,1:13],dat_test_word,dat_train[idx==k,(ncol(dat_train)-2):ncol(dat_train)])
pre.tmp = predict(clf.tmp,data.matrix(dat_test_fil[,-ncol(dat_test_fil)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=k)
}
plot(rev(lam),colMeans(err.tmp),"l",xlab = "lambda",ylab = "error rate")
for (k in 1:5) {
set.seed(k)
corr = apply(dat_train[idx!=k,14:(ncol(dat_train)-3)],2,cor,y=train_other$active_years[idx!=k])
if (any(is.na(corr))){
corr[is.na(corr)] = 0
}
idx_avail = abs(corr)>.025
dat_train_word = dat_train[idx!=k,idx_avail]
dat_train_fil = cbind(dat_train[idx!=k,1:13],dat_train_word,dat_train[idx!=k,(ncol(dat_train)-2):ncol(dat_train)])
clf.tmp = glmnet(data.matrix(dat_train_fil[,-ncol(dat_train_fil)]),as.factor(dat_train_fil[,ncol(dat_train_fil)]),
alpha = 0,lambda = lam,family = "multinomial")
dat_test_word = dat_train[idx==k,idx_avail]
dat_test_fil = cbind(dat_train[idx==k,1:13],dat_test_word,dat_train[idx==k,(ncol(dat_train)-2):ncol(dat_train)])
pre.tmp = predict(clf.tmp,data.matrix(dat_test_fil[,-ncol(dat_test_fil)]),type="class")
err.tmp[k,] = apply(pre.tmp,2,err_cal,k=k)
}
plot(rev(lam),colMeans(err.tmp),"l",xlab = "lambda",ylab = "error rate")
