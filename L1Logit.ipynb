{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "from gensim import models\n",
    "from scipy.sparse import lil_matrix, hstack, csr_matrix, vstack\n",
    "import gensim.downloader as api\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_392\\3642662710.py:47: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  df.loc[i,\"active_years\"] = int(ay_tmp)\n"
     ]
    }
   ],
   "source": [
    "def specific_preprocess(doc):\n",
    "    return simple_preprocess(doc,min_len=2)\n",
    "\n",
    "def remove_specific_words(s):\n",
    "    s = re.sub(r\"\\bLyrics\",\" \",s)\n",
    "    s = re.sub(r\"\\[.+\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\b\\d+\\b Contributors\",\" \",s)\n",
    "    s = re.sub(r\"Embed\",\" \",s)\n",
    "    s = re.sub(r\"You might also like\",\" \",s)\n",
    "    return s\n",
    "\n",
    "def count_space(s):\n",
    "    return s.count(' ')\n",
    "\n",
    "def remove_short_words(s):\n",
    "    s = re.sub(r\"\\b..\\b\",\" \",s)\n",
    "    s = re.sub(r\"\\b . \\b\",\" \",s)\n",
    "    pronoun = [r\"\\b you\\b\",r\"\\b yours\\b\",r\"\\b him \\b\",r\"\\b his\\b\", r\"\\b she \\b\", r\"\\b her \\b\", r\"\\b hers\\b\",\n",
    "               r\"\\b they \\b\", r\"\\b them \\b\", r\"\\b their \\b\", r\"\\b theirs \\b\",r\"\\b You\\b\",r\"\\b Yours\\b\",\n",
    "               r\"\\b Him \\b\",r\"\\b His\\b\", r\"\\b She \\b\", r\"\\b Her \\b\", r\"\\b Hers\\b\",\n",
    "               r\"\\b They \\b\", r\"\\b Them \\b\", r\"\\b Their \\b\", r\"\\b Theirs \\b\"]\n",
    "    conj = [r\"\\b and \\b\", r\"\\b then \\b\",r\"\\b for\\b\", r\"\\b from\\b\", r\"\\b with\\b\",\n",
    "            r\"\\b about\\b\",r\"\\b And \\b\", r\"\\b Then \\b\",r\"\\b For\\b\", r\"\\b From\\b\", r\"\\b With\\b\",\n",
    "            r\"\\b About\\b\"]\n",
    "    for word in pronoun:\n",
    "        s = re.sub(word,\" \",s)\n",
    "    for word in conj:\n",
    "        s = re.sub(word,\" \",s)\n",
    "    return s\n",
    "\n",
    "def count_lines(s):\n",
    "    res = len(re.findall(r\"\\r\\n\",s))\n",
    "    return res\n",
    "\n",
    "def count_paras(s):\n",
    "    res = len(re.findall(r\"\\r\\n\\r\\n\",s))\n",
    "    return res\n",
    "\n",
    "df = pd.read_csv(\"data/billboard_lyrics_genres.csv\")\n",
    "df_activeyear = pd.read_csv(\"data/first_active_years.csv\")\n",
    "df_activeyear = df_activeyear.drop_duplicates(subset=[\"band_singer\",\"title\",\"year\"],ignore_index=True)\n",
    "df[\"active_years\"] = 0\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    ay_tmp = df_activeyear.loc[(df_activeyear[\"band_singer\"]==df.loc[i,\"band_singer\"])&(df_activeyear[\"title\"]==df.loc[i,\"title\"])&(df_activeyear[\"year\"]==df.loc[i,\"year\"])].active_years\n",
    "    if not ay_tmp.empty:\n",
    "        df.loc[i,\"active_years\"] = int(ay_tmp)\n",
    "\n",
    "df_tmp = df.loc[df[\"active_years\"]!=0].reset_index(drop=True)\n",
    "\n",
    "df[\"numword\"] = df[\"lyrics\"].map(count_space)\n",
    "df[\"num_lines\"] = df[\"lyrics\"].map(count_lines)+1\n",
    "df[\"num_paras\"] = df[\"lyrics\"].map(count_paras)+1\n",
    "df[\"av_word_line\"] = df[\"numword\"]/df[\"num_lines\"]\n",
    "df[\"av_word_paras\"] = df[\"numword\"]/df[\"num_paras\"]\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(remove_specific_words)\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(remove_stopwords)\n",
    "# df[\"lyrics\"] = df[\"lyrics\"].map(remove_short_words)\n",
    "# df.to_csv(\"data/df_cluster.csv\")\n",
    "df[\"lyrics\"] = df[\"lyrics\"].map(specific_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_word(L):\n",
    "    unique_words = []\n",
    "    for word in L:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "    return len(unique_words)\n",
    "\n",
    "def find_max_len(L):\n",
    "    max_len = list(map(len,L))\n",
    "    return max(max_len)\n",
    "\n",
    "df[\"unique_words\"] = df[\"lyrics\"].map(find_unique_word)\n",
    "df[\"max_len\"] = df[\"lyrics\"].map(find_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(w):\n",
    "    return w.encode(\"utf-8\").isalpha()\n",
    "\n",
    "def isListEnglish(L):\n",
    "    return all(map(isEnglish,L))\n",
    "\n",
    "df[\"isEnglish\"] = df[\"lyrics\"].map(isListEnglish)\n",
    "df = df.loc[df[\"isEnglish\"],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pun(s):\n",
    "    s = re.sub(r\"\\[\\'\",\" \",s)\n",
    "    s = re.sub(r\"\\'\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\'\",\" \",s)\n",
    "    s = re.sub(r\"\\[\\]\",\" \",s)\n",
    "    s = re.sub(r\"\\,\",\" \",s)\n",
    "    s = s.split()\n",
    "    s = [token.lower() for token in s]\n",
    "    return s\n",
    "\n",
    "\n",
    "df[\"genre\"] = df[\"genre\"].map(remove_pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_des = [\"alternative\",\"country\",\"dance\",\"disco\",\"folk\",\"funk\",\"hip\",\"new\",\"pop\",\"r&b\",\"rap\",\"rock\",\"soul\"]\n",
    "gen_des = sorted(gen_des)\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = len(gen_des)\n",
    "dat_gen = lil_matrix((len(df), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    for word in row[\"genre\"]:\n",
    "        for k in range(len(gen_des)):\n",
    "            if re.search(gen_des[k],word):\n",
    "                dat_gen[i,k] = 1\n",
    "df[df[\"genre\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_gen = pd.DataFrame.sparse.from_spmatrix(dat_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = np.zeros(df.shape[0])\n",
    "\n",
    "bins = [1970,1980,1990,2000,2010,np.inf]\n",
    "\n",
    "labels = [0,1,2,3,4,5]\n",
    "\n",
    "df[\"label\"] = np.where(df[\"year\"] < bins[0], labels[0],\n",
    "                               np.where(df[\"year\"] < bins[1], labels[1],\n",
    "                                        np.where(df[\"year\"] < bins[2], labels[2],\n",
    "                                                 np.where(df[\"year\"] < bins[3], labels[3],\n",
    "                                                          np.where(df[\"year\"] < bins[4], labels[4], labels[5])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(515)\n",
    "idx = np.repeat(range(10),len(df.iloc[:,0])//10+1)\n",
    "df[\"idx\"] = np.random.choice(idx[range(len(df.iloc[:,0]))],size=len(df.iloc[:,0]))\n",
    "df_train = df.loc[df[\"idx\"]!=0,:]\n",
    "df_test = df.loc[df[\"idx\"]==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_392\\4121515207.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n"
     ]
    }
   ],
   "source": [
    "freq = defaultdict(int)\n",
    "for text in df_train[\"lyrics\"]:\n",
    "    for token in text:\n",
    "        freq[token] += 1\n",
    "\n",
    "processed_corpus = [[token for token in text if freq[token]>20] for text in df_train.loc[:,\"lyrics\"]]\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "df_train[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_train = lil_matrix((len(df_train), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"freq_count\"]]\n",
    "    values = [value for _, value in row[\"freq_count\"]]\n",
    "    dat_train[i, indices] = values\n",
    "df_train[df_train[\"freq_count\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_train = pd.DataFrame.sparse.from_spmatrix(dat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuzhi\\AppData\\Local\\Temp\\ipykernel_392\\2217317512.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n"
     ]
    }
   ],
   "source": [
    "df_test = df.loc[df[\"idx\"]==0,:]\n",
    "processed_corpus = [[token for token in text if freq[token]>20] for text in df_test.loc[:,\"lyrics\"]]\n",
    "df_test[\"freq_count\"] = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "# Compute number of columns from maximum word ID in the training data\n",
    "num_cols = max(dictionary.keys())+1\n",
    "dat_test = lil_matrix((len(df_test), num_cols), dtype=np.int64)\n",
    "\n",
    "# Fill in values using apply() and enumerate()\n",
    "def set_row_func(i, row):\n",
    "    indices = [count for count, word_id in row[\"freq_count\"] if count < num_cols]\n",
    "    values = [value for count, value in row[\"freq_count\"] if count < num_cols and value!=0]\n",
    "    dat_test[i, indices] = values\n",
    "df_test[df_test[\"freq_count\"].map(len) > 0].reset_index(drop=True).reset_index().apply(lambda row: set_row_func(row[\"index\"], row), axis=1)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "dat_test = pd.DataFrame.sparse.from_spmatrix(dat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_gen = dat_gen.reset_index()\n",
    "df = df.reset_index(drop=True)\n",
    "dat_gen_train = dat_gen.loc[df[\"idx\"]!=0,:].reset_index(drop=True)\n",
    "dat_gen_test = dat_gen.loc[df[\"idx\"]==0,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ones = csr_matrix(np.ones(df_train.shape[0])).transpose()\n",
    "test_ones = csr_matrix(np.ones(df_test.shape[0])).transpose()\n",
    "train_label = csr_matrix(df_train.loc[:,\"label\"]).transpose()\n",
    "test_label = csr_matrix(df_test.loc[:,\"label\"]).transpose()\n",
    "train_activeyear = csr_matrix(df_train.loc[:,\"active_years\":\"unique_words\"])\n",
    "test_activeyear = csr_matrix(df_test.loc[:,\"active_years\":\"unique_words\"])\n",
    "\n",
    "gen_train = csr_matrix(dat_gen_train.loc[:,0:])\n",
    "lyrics_train = csr_matrix(dat_train.loc[:,0:])\n",
    "data_train = hstack([train_ones,gen_train, lyrics_train,train_activeyear,train_label])\n",
    "data_train = pd.DataFrame.sparse.from_spmatrix(data_train)\n",
    "\n",
    "gen_test = csr_matrix(dat_gen_test.loc[:,0:])\n",
    "lyrics_test = csr_matrix(dat_test.loc[:,0:])\n",
    "data_test = hstack([test_ones,gen_test, lyrics_test,test_activeyear,test_label])\n",
    "data_test = pd.DataFrame.sparse.from_spmatrix(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_train.loc[:,\"active_years\":\"unique_words\"]\n",
    "word_name = [dictionary[i] for i in range(max(dictionary.keys())+1)]\n",
    "word_name = ['intercept']+gen_des + word_name +list(df_tmp.columns)+ ['label']\n",
    "data_train.columns = word_name\n",
    "data_test.columns = word_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "transformer = MaxAbsScaler()\n",
    "transformer.fit(data_train.iloc[:,:(data_train.shape[1]-1)])\n",
    "data_train_scaled = transformer.transform(data_train.iloc[:,:(data_train.shape[1]-1)])\n",
    "data_test_scaled = transformer.transform(data_test.iloc[:,:(data_train.shape[1]-1)])\n",
    "data_train_scaled = hstack([data_train_scaled,train_label])\n",
    "data_test_scaled = hstack([data_test_scaled,test_label])\n",
    "data_train_scaled = pd.DataFrame.sparse.from_spmatrix(data_train_scaled)\n",
    "data_test_scaled = pd.DataFrame.sparse.from_spmatrix(data_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5493562231759657\n",
      "1.7639484978540771\n"
     ]
    }
   ],
   "source": [
    "mr = LogisticRegression(penalty='l2',solver=\"liblinear\",max_iter=10000).fit(data_train_scaled.iloc[:,:(data_train_scaled.shape[1]-1)],np.array(df_train[\"label\"]))\n",
    "pred = mr.predict(data_test_scaled.iloc[:,:(data_test_scaled.shape[1]-1)])\n",
    "\n",
    "print(sum(pred == df_test[\"label\"])/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_coef_all = mr.coef_\n",
    "mr_coef_0 = np.argsort(mr_coef_all[0,14:(mr_coef_all.shape[1]-2)])+14\n",
    "mr_coef_0 = mr_coef_0[::-1]\n",
    "mr_word_0 = [word_name[mr_coef_0[i]] for i in range(100)]\n",
    "word_freq_0 = [round(mr_coef_all[0,mr_coef_0[i]]*100) for i in range(100)]\n",
    "word_freq_0 = {mr_word_0[i]:word_freq_0[i] for i in range(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud1 = WordCloud(background_color=\"white\")\n",
    "wordcloud0 = wordcloud1.generate_from_frequencies(word_freq_0)\n",
    "plt.imshow(wordcloud0,interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
